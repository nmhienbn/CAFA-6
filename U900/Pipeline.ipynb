{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "Running this notebook end-to-end will reproduce the solution. Step by step guide is also provided. You can skip some long running steps by executing corresponding cells of `Download.ipynb` to download artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_path: ./ # working dir\n",
      "cafa_data_path: ../data/raw/cafa6 # working dir\n",
      "# environments\n",
      "rapids-env: rapids-env/bin/python\n",
      "pytorch-env: pytorch-env/bin/python\n",
      "# artifacts paths\n",
      "embeds_path: ../features/embeds # path to embeddings \n",
      "models_path: ../models # store the models\n",
      "helpers_path: ../features/helpers # store reformated datasets\n",
      "temporal_path: ../features/temporal # store external data from FTP (temporal because different report dates are used)\n",
      "\n",
      "\n",
      "base_models: # all models and postprocessing path\n",
      "    pb_t5esm4500_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "            - esm_small\n",
      "        conditional: false\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t5esm4500_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "            - esm_small\n",
      "        conditional: true\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t54500_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: false\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t54500_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: true\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    lin_t5_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: false\n",
      "        bp: 10000\n",
      "        mf: 2000\n",
      "        cc: 1500\n",
      "        \n",
      "    lin_t5_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: true\n",
      "        bp: 10000\n",
      "        mf: 2000\n",
      "        cc: 1500\n",
      "        \n",
      "public_models: # models based on public script\n",
      "    nn_serg:\n",
      "        source: pytorch-keras-etc-3-blend-cafa-metric-etc.pkl\n",
      "        # source: test_nn.pkl\n",
      "        \n",
      "gcn: # stacking with graph neural network - separated by ontology\n",
      "    bp:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "\n",
      "    mf:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "                \n",
      "    cc:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n"
     ]
    }
   ],
   "source": [
    "!cat config.yaml\n",
    "\n",
    "with open('config.yaml') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "    \n",
    "BASE_PATH = CONFIG['base_path']\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, 'config.yaml')\n",
    "RAPIDS_ENV = os.path.join(BASE_PATH, CONFIG['rapids-env'])\n",
    "PYTORCH_ENV = os.path.join(BASE_PATH, CONFIG['pytorch-env'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "### 1.1. Setup envs\n",
    "\n",
    "Create the following python envs:\n",
    "\n",
    "* `pytorch-env` - env to deal with all DL models\n",
    "* `rapids-env`  - env to preprocess via RAPIDS and train py-boost and logregs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - rapidsai\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/hien/CAFA-6/U900/rapids-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - cuda-version=11.8\n",
      "    - cudatoolkit=11.8\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  cuda-version       conda-forge/noarch::cuda-version-11.8-h70ddcb2_3 \n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.8.0-h4ba93d1_13 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_16 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_16 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_16 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libstdcxx          conda-forge/linux-64::libstdcxx-15.2.0-h934c35e_16 \n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-15.2.0-hdf11a46_16 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-h5347b49_1 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.3-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.10.19-h3c07f61_2_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: \\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /data/hien/CAFA-6/U900/rapids-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting cupy-cuda11x\n",
      "  Downloading cupy_cuda11x-13.6.0-cp313-cp313-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting numba==0.56.4\n",
      "  Downloading numba-0.56.4.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[26 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-sj7c6pmf/numba_9f477b62ec86466c84488e9b76df48be/versioneer.py:415: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \u001b[31m   \u001b[0m   mo = re.search(r'=\\s*\"(.*)\"', line)\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m51\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m_guard_py_ver\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCannot install on Python version 3.13.5; only versions >=3.7,<3.11 are supported.\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'numba' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting cudf-cu11\n",
      "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuml-cu11\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cugraph-cu11\n",
      "  Downloading https://pypi.nvidia.com/cugraph-cu11/cugraph_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools (from cudf-cu11)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting cubinlinker-cu11 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuda-python<12.0a0,>=11.8.5 (from cudf-cu11)\n",
      "  Using cached cuda_python-11.8.7-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cupy-cuda11x>=12.0.0 (from cudf-cu11)\n",
      "  Using cached cupy_cuda11x-13.6.0-cp313-cp313-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /data/hien/.local/lib/python3.13/site-packages (from cudf-cu11) (2025.10.0)\n",
      "Collecting libcudf-cu11==25.6.* (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/libcudf-cu11/libcudf_cu11-25.6.0-py3-none-manylinux_2_28_x86_64.whl (445.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.9/445.9 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numba-cuda<0.12.0a0,>=0.11.0 (from cudf-cu11)\n",
      "  Using cached numba_cuda-0.11.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting numba<0.62.0a0,>=0.59.1 (from cudf-cu11)\n",
      "  Downloading numba-0.61.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /data/hien/.local/lib/python3.13/site-packages (from cudf-cu11) (2.3.5)\n",
      "Collecting nvtx>=0.2.1 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/nvtx/nvtx-0.2.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (717 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.8/717.8 kB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.13/site-packages (from cudf-cu11) (25.0)\n",
      "Collecting pandas<2.2.4dev0,>=2.0 (from cudf-cu11)\n",
      "  Downloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting ptxcompiler-cu11 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.8.1.post3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m6.6/8.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "/data/hien/CAFA-6/U900\n",
      "conda 25.7.0\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/hien/CAFA-6/U900/pytorch-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.9\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_16 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_16 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_16 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-h5347b49_1 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.2-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.9.23-hc30ae73_0_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /data/hien/CAFA-6/U900/pytorch-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 1002\n",
      "  gid: 1002\n",
      "\n",
      "\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 1002\n",
      "  gid: 1002\n",
      "\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /data/hien/.local/lib/python3.13/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.13/site-packages (6.0.3)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting numba==0.57.1\n",
      "  Downloading numba-0.57.1.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[24 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m51\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m_guard_py_ver\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCannot install on Python version 3.13.5; only versions >=3.8,<3.12 are supported.\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'numba' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: obonet in /data/hien/.local/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: pyvis in /data/hien/.local/lib/python3.13/site-packages (0.3.2)\n",
      "Requirement already satisfied: transformers in /data/hien/.local/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torchmetrics in /data/hien/.local/lib/python3.13/site-packages (1.8.2)\n",
      "Requirement already satisfied: torchsummary in /data/hien/.local/lib/python3.13/site-packages (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in /data/hien/.local/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (7.0.0)\n",
      "Requirement already satisfied: networkx in /data/hien/.local/lib/python3.13/site-packages (from obonet) (3.6)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.13/site-packages (from pyvis) (9.1.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.13/site-packages (from pyvis) (3.1.6)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /data/hien/.local/lib/python3.13/site-packages (from pyvis) (4.1.1)\n",
      "Requirement already satisfied: filelock in /data/hien/.local/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /data/hien/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /data/hien/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /data/hien/.local/lib/python3.13/site-packages (from torchmetrics) (2.9.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /data/hien/.local/lib/python3.13/site-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.13/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2>=2.9.6->pyvis) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (78.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.13/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/hien/.local/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# !./create-rapids-env.sh {BASE_PATH}\n",
    "# !./create-pytorch-env.sh {BASE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Get the input data\n",
    "\n",
    "Here we describe what should be stored in the working dir to reproduce the results\n",
    "\n",
    "Following data scheme was provided by Kaggle:\n",
    "\n",
    "    ./Train - cafa train data\n",
    "    ./Test (targets) - cafa test data\n",
    "    ./sample_submission.tsv - cafa ssub\n",
    "    ./IA.txt - cafa IA\n",
    "\n",
    "    \n",
    "Following are the solution code libraries, scipts, and notebooks used for training:\n",
    "\n",
    "    ./protlib\n",
    "    ./protnn\n",
    "    ./nn_solution\n",
    "    \n",
    "And the installed envs\n",
    "\n",
    "    ./pytorch-env\n",
    "    ./rapids-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Produce the helpers data\n",
    "\n",
    "First, we made some preprocessing of the input data to store everything in format that is convinient to us to handle and manipulate. Here is the structure:\n",
    "\n",
    "    ./helpers\n",
    "        ./fasta - fasta files stored as feather\n",
    "            ./train_seq.feather\n",
    "            ./test_seq.feather\n",
    "        ./real_targets - targets stored as n_proteins x n_terms parquet containing 0/1/NaN values\n",
    "            ./biological_process\n",
    "                ./part_0.parquet\n",
    "                ...\n",
    "                ./part_14.parquet\n",
    "                ./nulls.pkl - NaN rate of each term\n",
    "                ./priors.pkl - prior mean of each term (excluding NaN cells, like np.nanmean)\n",
    "            ./cellular_component\n",
    "            ./molecular_function\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845244it [00:00, 1013180.07it/s]\n",
      "1941130it [00:00, 2044347.95it/s]\n",
      "/data/hien/CAFA-6/U900\n",
      "Propagate:  True\n",
      "2it [09:49, 294.53s/it]\n",
      "CPU times: user 3.45 s, sys: 690 ms, total: 4.14 s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parse fasta files and save as feather\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_fasta.py \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "# convert targets to parquet and calculate priors\n",
    "# batch size 10000\n",
    "!{RAPIDS_ENV} protlib/scripts/create_helpers.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --batch-size 40000 \\\n",
    "    --propagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Get external data\n",
    "\n",
    "Datasets downloaded from outside and then processed. First step is downloading and parsing the datasets. After parsing, script will separate the datasets by the evidence codes. The most important split for us is kaggle/no-kaggle split. We refer `kaggle` as experimental codes, `no-kaggle` as electornic labeling, that will be used as features for the stacker models. Downloading takes quite a long time, while processing takes about 1 hour. The required structure after execution\n",
    "\n",
    "    ./temporal - extra data downloaded from http://ftp.ebi.ac.uk/pub/databases/GO/goa/old/UNIPROT/\n",
    "    ./labels   - extracted and propagated labeling\n",
    "        ./prop_test_leak_no_dup.tsv - leakage labeling\n",
    "        ./prop_test_no_kaggle.tsv   - electronic labels test\n",
    "        ./prop_train_no_kaggle.tsv  - electronic labels train\n",
    "        \n",
    "    ./cafa-terms-diff.tsv - reproduced difference between ML's dataset and our parsed labels\n",
    "    ./prop_quickgo51.tsv  - reproduced MT's quickgo 37 proteins\n",
    "    \n",
    "    \n",
    "Other files are temporary and not needed for future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110it [02:06,  1.14s/it]^C\n",
      "110it [02:07,  1.16s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/scripts/parse_go_single.py\", line 52, in <module>\n",
      "    for n, batch in tqdm.tqdm(enumerate(reader)):\n",
      "  File \"/data/hien/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1843, in __next__\n",
      "    return self.get_chunk()\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1985, in get_chunk\n",
      "    return self.read(nrows=size)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "  File \"parsers.pyx\", line 850, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
      "  File \"parsers.pyx\", line 2053, in pandas._libs.parsers.raise_parser_error\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/gzip.py\", line 510, in read\n",
      "    self._add_read_data( uncompress )\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/gzip.py\", line 515, in _add_read_data\n",
      "    self._crc = zlib.crc32(data, self._crc)\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/scripts/parse_go_single.py\", line 37, in <module>\n",
      "    idxs = train_idx.union(test_idx)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# download external data from ebi.ac.uk\n",
    "# !{RAPIDS_ENV} protlib/scripts/downloads/dw_goant.py \\\n",
    "#     --config-path {CONFIG_PATH}\n",
    "\n",
    "# # parse the files\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.226.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output ver226\n",
    "\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.228.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output ver228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is propagation. Since ebi.ac datasets contains the labeling without propagation, we will apply the rules provided in organizer's repo to labeling more terms. We will do it only for `goa_uniprot_all.gaf.216.gz` datasets since it is the actual dataset at the active competition phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 33%|███████████████                              | 1/3 [00:13<00:26, 13.49s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 67%|██████████████████████████████               | 2/3 [00:24<00:11, 11.83s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:32<00:00, 10.93s/it]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 12%|█████▋                                       | 1/8 [00:11<01:18, 11.18s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 25%|███████████▎                                 | 2/8 [00:21<01:05, 10.91s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 38%|████████████████▉                            | 3/8 [00:29<00:46,  9.31s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 50%|██████████████████████▌                      | 4/8 [00:38<00:37,  9.42s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 62%|████████████████████████████▏                | 5/8 [00:47<00:27,  9.21s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 75%|█████████████████████████████████▊           | 6/8 [00:56<00:18,  9.11s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [01:04<00:08,  8.71s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "100%|█████████████████████████████████████████████| 8/8 [01:07<00:00,  8.48s/it]\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "folder = BASE_PATH + '/temporal/ver228'\n",
    "\n",
    "for file in glob.glob(folder + '/labels/train*') + glob.glob(folder + '/labels/test*'):\n",
    "    name = folder + '/labels/prop_' + file.split('/')[-1]\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "        --path {file} \\\n",
    "        --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "        --output {name} \\\n",
    "        --device 0 \\\n",
    "        --batch_size 30000 \\\n",
    "        --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is reproducing MT's datasets that are commonly used in all public kernels. We didn't use it directly, but we used `cafa-terms-diff` dataset, that represents the difference between our labeling obtained by parsing `goa_uniprot_all.gaf.216.gz` dataset and `all_dict.pkl` dataset given by MT. As he claims in the dicussion [here](https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/404853#2329935) he used the same FTP source as we. But our source is more actual than the public. So the difference is actually the temporal. After analysis, we find out, that we are able to reproduce it as the difference between `goa_uniprot_all.gaf.216.gz` and `goa_uniprot_all.gaf.214.gz` sources. So, we just create `cafa-terms-diff` dataset by the given script. The only difference between the source in the kaggle script and used here is deduplication. We removed duplicated protein/terms pairs from the dataset, it has almost zero impact on the metric value (less than 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hien/CAFA-6/U900\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 33%|███████████████                              | 1/3 [00:13<00:26, 13.43s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      " 67%|██████████████████████████████               | 2/3 [00:26<00:13, 13.24s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py:73: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), 1)\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:35<00:00, 11.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/reproduce_mt.py \\\n",
    "    --path {BASE_PATH}/temporal/ver228 \\\n",
    "    --old-path {BASE_PATH}/temporal/ver226 \\\n",
    "    --new-path {BASE_PATH}/temporal/ver228 \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --target {BASE_PATH}/embeds/esm_small/train_ids.npy\n",
    "\n",
    "# # make propagation for quickgo51.tsv\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "    --path {BASE_PATH}/temporal/ver228/quickgo51.tsv \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --output {BASE_PATH}/temporal/ver228/prop_quickgo51.tsv \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Preparation step for neural networks\n",
    "\n",
    "Produce some helpers to train NN model. Creates the following data:\n",
    "\n",
    "    ./helpers/feats\n",
    "        ./train_ids_cut43k.npy\n",
    "        ./Y_31466_labels.npy\n",
    "        ./Y_31466_sparse_float32.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537027, 3)\n",
      "term\n",
      "GO:0005515    33713\n",
      "GO:0005634    13283\n",
      "GO:0005829    13040\n",
      "GO:0005886    10150\n",
      "GO:0005737     9442\n",
      "              ...  \n",
      "GO:0010622        1\n",
      "GO:0042357        1\n",
      "GO:0009421        1\n",
      "GO:0008764        1\n",
      "GO:0061336        1\n",
      "Name: count, Length: 26125, dtype: int64\n",
      "aspect\n",
      "BPO    16858\n",
      "CCO     2651\n",
      "MFO     6616\n",
      "Name: term, dtype: int64\n",
      "CPU times: user 8.97 ms, sys: 9 ms, total: 18 ms\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/prepare.py \\\n",
    "    --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 T5 pretrained inference\n",
    "\n",
    "    ./embeds\n",
    "        ./t5\n",
    "            ./train_ids.npy\n",
    "            ./train_embeds.npy\n",
    "            ./test_ids.npy\n",
    "            ./test_embeds.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 82404/82404 [00:00<00:00, 1586618.68it/s]\n",
      "Number of sequences in test: 224309\n",
      "100%|██████████████████████████████| 224309/224309 [00:00<00:00, 1409253.02it/s]\n",
      "CPU times: user 101 ms, sys: 53.7 ms, total: 155 ms\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/t5.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ESM pretrained inference\n",
    "\n",
    "    ./embeds\n",
    "        ./esm_small\n",
    "            ./train_ids.npy\n",
    "            ./train_embeds.npy\n",
    "            ./test_ids.npy\n",
    "            ./test_embeds.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 82404/82404 [00:00<00:00, 1396609.88it/s]\n",
      "Number of sequences in test: 224309\n",
      "100%|██████████████████████████████| 224309/224309 [00:00<00:00, 1110830.53it/s]\n",
      "CPU times: user 87.3 ms, sys: 171 ms, total: 258 ms\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/esm2sm.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train and inference py-boost models\n",
    "\n",
    "GBDT models description:\n",
    "\n",
    "1) Features: T5 + taxon, targets: multilabel\n",
    "\n",
    "2) Features: T5 + taxon, targets: conditional\n",
    "\n",
    "3) Features: T5 + ESM + taxon, targets: multilabel\n",
    "\n",
    "4) Features: T5 + ESM + taxon, targets: conditional\n",
    "\n",
    "Pipeline and hyperparameters are the same for all the models. Target is 4500 output: BP 3000, MF: 1000, CC: 500. All models could be ran in parallel to save a time. We used single V100 32GB and it requires about 15 hours to train 5 fold CV loop for each model type. 32GB GPU RAM is required, otherwise OOM will occur. Structure is:\n",
    "    \n",
    "    ./models\n",
    "        ./pb_t54500_raw\n",
    "            ./models_0.pkl\n",
    "            ...\n",
    "            ./models_4.pkl\n",
    "            ./oof_pred.pkl\n",
    "            ./test_pred.pkl\n",
    "        ./pb_t54500_cond\n",
    "            ...\n",
    "        ./pb_t5esm4500_raw\n",
    "            ...\n",
    "        ./pb_t5esm4500_cond\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pb_t54500_raw\n",
      "True\n",
      "trg filled\n",
      "trg filled\n",
      "trg filled\n",
      "(82404, 1056) (224309, 1056)\n",
      "(65906,) (16498,)\n",
      "[18:51:25] Stdout logging level is INFO.\n",
      "[18:51:25] GDBT train starts. Max iter 20000, early stopping rounds 300\n",
      "[18:51:29] Iter 0; Sample 0, BCE = 0.01847760927347621; \n",
      "[18:51:45] Iter 100; Sample 0, BCE = 0.016180626734453446; \n",
      "[18:52:01] Iter 200; Sample 0, BCE = 0.015253586837214339; \n",
      "[18:52:17] Iter 300; Sample 0, BCE = 0.014672117275444232; \n",
      "[18:52:33] Iter 400; Sample 0, BCE = 0.014263558791376245; \n",
      "[18:52:49] Iter 500; Sample 0, BCE = 0.013957509020161131; \n",
      "[18:53:05] Iter 600; Sample 0, BCE = 0.013711631127375232; \n",
      "[18:53:22] Iter 700; Sample 0, BCE = 0.013512374108627436; \n",
      "[18:53:38] Iter 800; Sample 0, BCE = 0.013347417028317163; \n",
      "[18:53:54] Iter 900; Sample 0, BCE = 0.013204780972392393; \n",
      "[18:54:10] Iter 1000; Sample 0, BCE = 0.013082082929463185; \n",
      "[18:54:26] Iter 1100; Sample 0, BCE = 0.012978140873277224; \n",
      "[18:54:42] Iter 1200; Sample 0, BCE = 0.012886041055240865; \n",
      "[18:54:58] Iter 1300; Sample 0, BCE = 0.012799471583522785; \n",
      "[18:55:14] Iter 1400; Sample 0, BCE = 0.012722024085886692; \n",
      "[18:55:30] Iter 1500; Sample 0, BCE = 0.01265154256554801; \n",
      "[18:55:46] Iter 1600; Sample 0, BCE = 0.012587806795296898; \n",
      "[18:56:02] Iter 1700; Sample 0, BCE = 0.0125307681506698; \n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/.//protlib/scripts/train_pb.py\", line 129, in <module>\n",
      "    model.fit(X[tr_idx], Y[tr_idx], eval_sets=[{'X': X[ts_idx], 'y': Y[ts_idx]}])\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/py_boost/gpu/boosting.py\", line 267, in fit\n",
      "    self._fit(builder, build_info)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/py_boost/gpu/boosting.py\", line 209, in _fit\n",
      "    builder.build_tree(train['features_gpu'],\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/py_boost/gpu/tree.py\", line 548, in build_tree\n",
      "    train_nodes, valid_nodes = depthwise_grow_tree(tree, n_grp, X, G, H,\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/py_boost/gpu/utils.py\", line 859, in depthwise_grow_tree\n",
      "    get_cpu_splitters(unique_nodes, best_feat, best_gain, best_split, best_nan_left,\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/py_boost/gpu/utils.py\", line 775, in get_cpu_splitters\n",
      "    best_gain = best_gain.get()\n",
      "KeyboardInterrupt\n",
      "Training pb_t54500_cond\n",
      "^C\n",
      "Training pb_t5esm4500_raw\n",
      "^C\n",
      "Training pb_t5esm4500_cond\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['pb_t54500_raw', 'pb_t54500_cond', 'pb_t5esm4500_raw', 'pb_t5esm4500_cond', ]:\n",
    "\n",
    "    print(f'Training {model_name}')\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/train_pb.py \\\n",
    "        --config-path {CONFIG_PATH} \\\n",
    "        --model-name {model_name} \\\n",
    "        --device 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train and inference logreg models\n",
    "\n",
    "Logistic Regression models description:\n",
    "\n",
    "1) Features: T5 + taxon, targets: multilabel\n",
    "\n",
    "2) Features: T5 + taxon, targets: conditional\n",
    "\n",
    "\n",
    "Pipeline and hyperparameters are the same for all the models. Target is 13500 output: BP 10000, MF: 2000, CC: 1500. All models could be ran in parallel to save a time. We used single V100 32GB and it requires about 10 hours for model 1 and 2 hours for model 2 to train 5 fold CV loop. 32GB GPU RAM is required, otherwise OOM will occur. Structure is:\n",
    "\n",
    "    ./helpers\n",
    "        ./folds_gkf.npy\n",
    "    ./models\n",
    "        ./lin_t5_raw\n",
    "            ./models_0.pkl\n",
    "            ...\n",
    "            ./models_4.pkl\n",
    "            ./oof_pred.pkl\n",
    "            ./test_pred.pkl\n",
    "        ./lin_t5_cond\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lin_t5_raw\n",
      "True\n",
      "trg filled\n",
      "trg filled\n",
      "trg filled\n",
      "(82404, 1056) (224309, 1056)\n",
      "(65906,) (16498,)\n",
      "  1%|▌                                        | 2/135 [00:56<1:01:46, 27.87s/it]^C\n",
      "  1%|▌                                        | 2/135 [01:14<1:23:04, 37.48s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/.//protlib/scripts/train_lin.py\", line 121, in <module>\n",
      "    model.fit(X[tr_idx], Y[tr_idx])\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/models/logreg.py\", line 67, in fit\n",
      "    delta[:, k] = cp.dot(cp.linalg.inv(hess), grad[:, k])\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupy/linalg/_solve.py\", line 261, in inv\n",
      "    cupyx.lapack.gesv(a.T, b.T)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/lapack.py\", line 69, in gesv\n",
      "    getrf(handle, n, n, a.data.ptr, n, dwork.data.ptr, dipiv.data.ptr,\n",
      "KeyboardInterrupt\n",
      "Training lin_t54500_cond\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "for model_name in ['lin_t5_raw', 'lin_t54500_cond']:\n",
    "\n",
    "    print(f'Training {model_name}')\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/train_lin.py \\\n",
    "        --config-path {CONFIG_PATH} \\\n",
    "        --model-name {model_name} \\\n",
    "        --device 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train and inference NN models\n",
    "\n",
    "Structure is:\n",
    "\n",
    "    ./models\n",
    "        ./nn_serg\n",
    "            ./model_0_0.pt\n",
    "            ...\n",
    "            ./model_11_4.pt\n",
    "            ./pytorch-keras-etc-3-blend-cafa-metric-etc.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create train folds (the same as used for pb_t54500_cond model)\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/create_gkf.py \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "# train models\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/train_models.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0\n",
    "\n",
    "# inference models\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/inference_models.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0\n",
    "\n",
    "# reformat to use in stack\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/make_pkl.py \\\n",
    "    --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final model\n",
    "\n",
    "### 4.1. Train GCN models\n",
    "\n",
    "This step is training 3 independent stacking models for each ontology. Models are trained on single V100 GPU and it takes about 13 hours for BP, 4 hours for MF and 2 hours for CC. 32 GB GPU RAM is required to fit. Could be trained in parallel if 2 GPUs are avaliable - BP and MF/CC. Structure:\n",
    "\n",
    "    ./models\n",
    "        ./gcn\n",
    "            ./bp\n",
    "                ./checkpoint.pth\n",
    "            ./mf\n",
    "                ./checkpoint.pth\n",
    "            ./cc\n",
    "                ./checkpoint.pth\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ont in ['bp', 'mf', 'cc']:\n",
    "    !{PYTORCH_ENV} {BASE_PATH}/protnn/scripts/train_gcn.py \\\n",
    "        --config-path {CONFIG_PATH} \\\n",
    "        --ontology {ont} \\\n",
    "        --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inference GCN models and TTA\n",
    "\n",
    "Inference and Test-Time-Augmentation. Structure:\n",
    "\n",
    "    ./models\n",
    "        ./gcn\n",
    "            ./pred_tta_0.tsv\n",
    "            ...\n",
    "            ./pred_tta_3.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['./models/pb_t54500_cond', [3000, 1000, 500], True], ['./models/pb_t54500_raw', [3000, 1000, 500], False], ['./models/lin_t5_cond', [10000, 2000, 1500], True], ['./models/lin_t5_raw', [10000, 2000, 1500], False]]\n",
      "  0%|                                                  | 0/1753 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/protnn/stacker.py:17: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/Indexing.cu:1454.)\n",
      "  x0.index_reduce(1, dst, x0[:, src], reduce='mean', include_self=False),\n",
      "100%|███████████████████████████████████████| 1753/1753 [23:34<00:00,  1.24it/s]\n",
      "[['./models/pb_t5esm4500_cond', [3000, 1000, 500], True], ['./models/pb_t54500_raw', [3000, 1000, 500], False], ['./models/lin_t5_cond', [10000, 2000, 1500], True], ['./models/lin_t5_raw', [10000, 2000, 1500], False]]\n",
      " 74%|████████████████████████████▉          | 1300/1753 [17:45<06:03,  1.25it/s]^C\n",
      "CPU times: user 14.8 s, sys: 3.35 s, total: 18.1 s\n",
      "Wall time: 42min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/protnn/scripts/predict_gcn.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Postprocessing and build submission file\n",
    "\n",
    "Here we do the following:\n",
    "\n",
    "1) Average TTA predictions\n",
    "2) Perform min prop\n",
    "3) Perform max prop\n",
    "4) Average min/max prop steps, add external leakage data and make submission\n",
    "\n",
    "Structure:\n",
    "\n",
    "    ./models\n",
    "        ./postproc\n",
    "            ./pred.tsv     - avg TTA\n",
    "            ./pred_min.tsv - min prop\n",
    "            ./pred_max.tsv - max prop\n",
    "            \n",
    "    ./sub\n",
    "        ./submission.tsv   - final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 0           O14503\n",
      "1           O14503\n",
      "2           O14503\n",
      "3           O14503\n",
      "4           O14503\n",
      "             ...  \n",
      "27772794    Q08118\n",
      "27774851    Q8MKL1\n",
      "27774852    Q8VE95\n",
      "27774853    Q8CDV0\n",
      "27774854    Q8GWI2\n",
      "Name: entry_id, Length: 19852943, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A098D1J7        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4IB93        2\n",
      "A0A1D8PDP8        3\n",
      "A0A1W2P7I0        4\n",
      "              ...  \n",
      "Q9ZW22        79026\n",
      "Q9ZW81        79027\n",
      "R9QMR2        79028\n",
      "T2HG31        79029\n",
      "W8JIS5        79030\n",
      "Name: index, Length: 79031, dtype: int64\n",
      " - Tổng số dòng dự đoán: 19852943\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:15, 15.95s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:27, 13.21s/it]                            | 1/3 [00:12<00:25, 12.80s/it]\u001b[A\n",
      "3it [00:38, 12.48s/it]█████████████               | 2/3 [00:24<00:11, 11.91s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:35<00:00, 11.90s/it]\u001b[A\n",
      "3it [00:38, 12.98s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 19546170    O00116\n",
      "19546171    O00116\n",
      "19546172    O00116\n",
      "19546173    O00116\n",
      "19546174    O00116\n",
      "             ...  \n",
      "27686728    Q6L4L4\n",
      "27686729    Q6NMK6\n",
      "27686730    Q80T69\n",
      "27686731    Q8BJT9\n",
      "27686732    Q6L711\n",
      "Name: entry_id, Length: 1649211, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0G2KBC9        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4INB9        2\n",
      "A0A1P8BH59        3\n",
      "A0A3F2YLV2        4\n",
      "              ...  \n",
      "Q9ZW81        78906\n",
      "Q9ZWJ3        78907\n",
      "R9QMR2        78908\n",
      "T2HG31        78909\n",
      "W8JIS5        78910\n",
      "Name: index, Length: 78911, dtype: int64\n",
      " - Tổng số dòng dự đoán: 1649211\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:08,  8.54s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:14,  7.31s/it]                            | 1/3 [00:05<00:11,  5.71s/it]\u001b[A\n",
      "3it [00:20,  6.28s/it]█████████████               | 2/3 [00:12<00:06,  6.14s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:17<00:00,  5.73s/it]\u001b[A\n",
      "3it [00:20,  6.69s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 21258509    B1AK53\n",
      "21258510    B1AK53\n",
      "21258511    B1AK53\n",
      "21258512    B1AK53\n",
      "21258513    B1AK53\n",
      "             ...  \n",
      "27897706    O74737\n",
      "27897707    O94384\n",
      "27897708    O94675\n",
      "27897709    O94683\n",
      "27897710    O94721\n",
      "Name: entry_id, Length: 6405020, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0P0XII1        0\n",
      "A0A1D8PDP8        1\n",
      "A0A286ZK88        2\n",
      "A0A287B8J2        3\n",
      "A0A2R8Y4L2        4\n",
      "              ...  \n",
      "Q9ZW81        79041\n",
      "Q9ZWJ3        79042\n",
      "R9QMR2        79043\n",
      "T2HG31        79044\n",
      "W8JIS5        79045\n",
      "Name: index, Length: 79046, dtype: int64\n",
      " - Tổng số dòng dự đoán: 6405020\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:08,  8.69s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:14,  7.22s/it]                            | 1/3 [00:05<00:10,  5.15s/it]\u001b[A\n",
      "3it [00:20,  6.49s/it]█████████████               | 2/3 [00:11<00:05,  5.76s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:16<00:00,  5.65s/it]\u001b[A\n",
      "3it [00:20,  6.86s/it]\n",
      "CAFA5 Scores\n",
      "{'bp': 0.6940243583437545, 'mf': 0.8322372288968939, 'cc': 0.7625223059447835, 'cafa': np.float64(0.762927964395144)}\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 12%|█████▋                                       | 1/8 [00:53<06:14, 53.44s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 25%|███████████▎                                 | 2/8 [01:39<04:52, 48.83s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 38%|████████████████▉                            | 3/8 [02:19<03:44, 44.91s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 50%|██████████████████████▌                      | 4/8 [02:57<02:48, 42.14s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 62%|████████████████████████████▏                | 5/8 [03:33<02:00, 40.16s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 75%|█████████████████████████████████▊           | 6/8 [04:09<01:17, 38.78s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [04:43<00:37, 37.13s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "100%|█████████████████████████████████████████████| 8/8 [05:03<00:00, 37.94s/it]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 12%|█████▋                                       | 1/8 [00:51<06:02, 51.80s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 25%|███████████▎                                 | 2/8 [01:37<04:48, 48.09s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 38%|████████████████▉                            | 3/8 [02:15<03:36, 43.37s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 50%|██████████████████████▌                      | 4/8 [02:47<02:36, 39.09s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 62%|████████████████████████████▏                | 5/8 [03:20<01:50, 36.80s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 75%|█████████████████████████████████▊           | 6/8 [03:51<01:09, 34.93s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [04:21<00:33, 33.36s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "100%|█████████████████████████████████████████████| 8/8 [04:37<00:00, 34.70s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 0           A6BM72\n",
      "1           A6BM72\n",
      "2           A6BM72\n",
      "3           A6BM72\n",
      "4           A6BM72\n",
      "             ...  \n",
      "29819053    Q9HGP9\n",
      "29819054    Q9HGP9\n",
      "29819055    Q9HGP9\n",
      "29819056    Q9HGP9\n",
      "29819057    Q9HGP9\n",
      "Name: entry_id, Length: 20951042, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A098D1J7        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4IB93        2\n",
      "A0A1D8PDP8        3\n",
      "A0A1W2P7I0        4\n",
      "              ...  \n",
      "Q9ZW22        79026\n",
      "Q9ZW81        79027\n",
      "R9QMR2        79028\n",
      "T2HG31        79029\n",
      "W8JIS5        79030\n",
      "Name: index, Length: 79031, dtype: int64\n",
      " - Tổng số dòng dự đoán: 20951042\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:14, 14.35s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:25, 12.43s/it]                            | 1/3 [00:11<00:22, 11.44s/it]\u001b[A\n",
      "3it [00:36, 11.61s/it]█████████████               | 2/3 [00:22<00:11, 11.23s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:33<00:00, 11.05s/it]\u001b[A\n",
      "3it [00:36, 12.02s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 9275682     O14503\n",
      "9275683     O14503\n",
      "9275684     O14503\n",
      "9275685     O14503\n",
      "9275686     O14503\n",
      "             ...  \n",
      "29702604    Q9URV4\n",
      "29702605    Q9URX2\n",
      "29702606    Q9URV4\n",
      "29702607    Q9URV4\n",
      "29702608    Q9URV4\n",
      "Name: entry_id, Length: 1732311, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0G2KBC9        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4INB9        2\n",
      "A0A1P8BH59        3\n",
      "A0A3F2YLV2        4\n",
      "              ...  \n",
      "Q9ZW81        78906\n",
      "Q9ZWJ3        78907\n",
      "R9QMR2        78908\n",
      "T2HG31        78909\n",
      "W8JIS5        78910\n",
      "Name: index, Length: 78911, dtype: int64\n",
      " - Tổng số dòng dự đoán: 1732311\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:08,  8.39s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:14,  6.84s/it]                            | 1/3 [00:05<00:11,  5.52s/it]\u001b[A\n",
      "3it [00:19,  6.15s/it]█████████████               | 2/3 [00:11<00:05,  5.66s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:16<00:00,  5.54s/it]\u001b[A\n",
      "3it [00:19,  6.55s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 9982189     A1A4S6\n",
      "9982190     A1A4S6\n",
      "9982191     A1A4S6\n",
      "9982192     A1A4S6\n",
      "9982193     A1A4S6\n",
      "             ...  \n",
      "29961015    Q9US01\n",
      "29961016    Q9US01\n",
      "29961017    Q9USI0\n",
      "29961018    Q9USI0\n",
      "29961019    Q9USI0\n",
      "Name: entry_id, Length: 7292272, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0P0XII1        0\n",
      "A0A1D8PDP8        1\n",
      "A0A286ZK88        2\n",
      "A0A287B8J2        3\n",
      "A0A2R8Y4L2        4\n",
      "              ...  \n",
      "Q9ZW81        79041\n",
      "Q9ZWJ3        79042\n",
      "R9QMR2        79043\n",
      "T2HG31        79044\n",
      "W8JIS5        79045\n",
      "Name: index, Length: 79046, dtype: int64\n",
      " - Tổng số dòng dự đoán: 7292272\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:08,  8.01s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:13,  6.46s/it]                            | 1/3 [00:05<00:10,  5.22s/it]\u001b[A\n",
      "3it [00:18,  5.81s/it]█████████████               | 2/3 [00:10<00:05,  5.31s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.21s/it]\u001b[A\n",
      "3it [00:18,  6.21s/it]\n",
      "CAFA5 Scores\n",
      "{'bp': 0.6955805764630006, 'mf': 0.8328467188338399, 'cc': 0.769111732798096, 'cafa': np.float64(0.7658463426983122)}\n"
     ]
    }
   ],
   "source": [
    "# since we have 4 TTA predictions, we need to aggregate all as an average\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/collect_ttas.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0\n",
    "\n",
    "# create 0.3 * pred + 0.7 * max children propagation\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/step.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 3000 \\\n",
    "    --lr 0.7 \\\n",
    "    --direction min\n",
    "\n",
    "# create 0.3 * pred + 0.7 * min parents propagation\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/step.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 3000 \\\n",
    "    --lr 0.7 \\\n",
    "    --direction max\n",
    "\n",
    "# here we average min prop and max prop solutions, mix with cafa-terms-diff and quickgo51 datasets from 1.4\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/make_submission.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --max-rate 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "Result is stored in `./sub/submission.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6DER1\tGO:0005654\t0.066\n",
      "Q6B8W6\tGO:0031974\t0.063\n",
      "P54956\tGO:0051179\t0.012\n",
      "O02747\tGO:1900428\t0.009\n",
      "Q66HF8\tGO:0006720\t0.025\n",
      "Q09265\tGO:0019538\t0.004\n",
      "O01835\tGO:0010605\t0.636\n",
      "Q4R4R0\tGO:0043933\t0.028\n",
      "A5DNX9\tGO:0010605\t0.023\n",
      "B4KA44\tGO:0051234\t0.024\n"
     ]
    }
   ],
   "source": [
    "!head {BASE_PATH}/sub/submission.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cafa)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
