{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware requirements\n",
    "- GPU: 8 x A100 80GB\n",
    "- CPU: AMD 2TB, 256 cores\n",
    "- Disk Storage: 2TB (At least 300GB for this project)\n",
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_path: ./ # working dir\n",
      "cafa_data_path: ../data/raw/cafa6 # working dir\n",
      "# environments\n",
      "rapids-env: rapids-env/bin/python\n",
      "pytorch-env: pytorch-env/bin/python\n",
      "# artifacts paths\n",
      "embeds_path: ../features/embeds # path to embeddings \n",
      "models_path: ../models # store the models\n",
      "helpers_path: ../features/helpers # store reformated datasets\n",
      "temporal_path: ../features/temporal # store external data from FTP (temporal because different report dates are used)\n",
      "\n",
      "\n",
      "base_models: # all models and postprocessing path\n",
      "    pb_t5esm4500_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "            - esm_small\n",
      "        conditional: false\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t5esm4500_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "            - esm_small\n",
      "        conditional: true\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t54500_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: false\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t54500_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: true\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    lin_t5_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: false\n",
      "        bp: 10000\n",
      "        mf: 2000\n",
      "        cc: 1500\n",
      "        \n",
      "    lin_t5_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: true\n",
      "        bp: 10000\n",
      "        mf: 2000\n",
      "        cc: 1500\n",
      "        \n",
      "public_models: # models based on public script\n",
      "    nn_serg:\n",
      "        source: pytorch-keras-etc-3-blend-cafa-metric-etc.pkl\n",
      "        # source: test_nn.pkl\n",
      "        \n",
      "gcn: # stacking with graph neural network - separated by ontology\n",
      "    bp:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "\n",
      "    mf:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "                \n",
      "    cc:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n"
     ]
    }
   ],
   "source": [
    "!cat config.yaml\n",
    "\n",
    "with open('config.yaml') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "    \n",
    "BASE_PATH = CONFIG['base_path']\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, 'config.yaml')\n",
    "RAPIDS_ENV = os.path.join(BASE_PATH, CONFIG['rapids-env'])\n",
    "PYTORCH_ENV = os.path.join(BASE_PATH, CONFIG['pytorch-env'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "### 1.1. Setup envs\n",
    "\n",
    "Create the following python envs:\n",
    "\n",
    "* `pytorch-env` - env to deal with all DL models\n",
    "* `rapids-env`  - env to preprocess via RAPIDS and train py-boost and logregs\n",
    "\n",
    "**You should run this outside terminal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - rapidsai\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/hien/CAFA-6/U900/rapids-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - cuda-version=11.8\n",
      "    - cudatoolkit=11.8\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  cuda-version       conda-forge/noarch::cuda-version-11.8-h70ddcb2_3 \n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.8.0-h4ba93d1_13 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_16 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_16 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_16 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libstdcxx          conda-forge/linux-64::libstdcxx-15.2.0-h934c35e_16 \n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-15.2.0-hdf11a46_16 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-h5347b49_1 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.3-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.10.19-h3c07f61_2_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: \\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /data/hien/CAFA-6/U900/rapids-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting cupy-cuda11x\n",
      "  Downloading cupy_cuda11x-13.6.0-cp313-cp313-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting numba==0.56.4\n",
      "  Downloading numba-0.56.4.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[26 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-sj7c6pmf/numba_9f477b62ec86466c84488e9b76df48be/versioneer.py:415: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \u001b[31m   \u001b[0m   mo = re.search(r'=\\s*\"(.*)\"', line)\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m51\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m_guard_py_ver\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCannot install on Python version 3.13.5; only versions >=3.7,<3.11 are supported.\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'numba' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting cudf-cu11\n",
      "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuml-cu11\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cugraph-cu11\n",
      "  Downloading https://pypi.nvidia.com/cugraph-cu11/cugraph_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools (from cudf-cu11)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting cubinlinker-cu11 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuda-python<12.0a0,>=11.8.5 (from cudf-cu11)\n",
      "  Using cached cuda_python-11.8.7-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cupy-cuda11x>=12.0.0 (from cudf-cu11)\n",
      "  Using cached cupy_cuda11x-13.6.0-cp313-cp313-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /data/hien/.local/lib/python3.13/site-packages (from cudf-cu11) (2025.10.0)\n",
      "Collecting libcudf-cu11==25.6.* (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/libcudf-cu11/libcudf_cu11-25.6.0-py3-none-manylinux_2_28_x86_64.whl (445.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.9/445.9 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numba-cuda<0.12.0a0,>=0.11.0 (from cudf-cu11)\n",
      "  Using cached numba_cuda-0.11.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting numba<0.62.0a0,>=0.59.1 (from cudf-cu11)\n",
      "  Downloading numba-0.61.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /data/hien/.local/lib/python3.13/site-packages (from cudf-cu11) (2.3.5)\n",
      "Collecting nvtx>=0.2.1 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/nvtx/nvtx-0.2.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (717 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.8/717.8 kB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.13/site-packages (from cudf-cu11) (25.0)\n",
      "Collecting pandas<2.2.4dev0,>=2.0 (from cudf-cu11)\n",
      "  Downloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting ptxcompiler-cu11 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.8.1.post3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m6.6/8.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "/data/hien/CAFA-6/U900\n",
      "conda 25.7.0\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/hien/CAFA-6/U900/pytorch-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.9\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_16 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_16 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_16 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-h5347b49_1 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.2-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.9.23-hc30ae73_0_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /data/hien/CAFA-6/U900/pytorch-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 1002\n",
      "  gid: 1002\n",
      "\n",
      "\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 1002\n",
      "  gid: 1002\n",
      "\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /data/hien/.local/lib/python3.13/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.13/site-packages (6.0.3)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting numba==0.57.1\n",
      "  Downloading numba-0.57.1.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[24 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m51\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m_guard_py_ver\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCannot install on Python version 3.13.5; only versions >=3.8,<3.12 are supported.\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'numba' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: obonet in /data/hien/.local/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: pyvis in /data/hien/.local/lib/python3.13/site-packages (0.3.2)\n",
      "Requirement already satisfied: transformers in /data/hien/.local/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torchmetrics in /data/hien/.local/lib/python3.13/site-packages (1.8.2)\n",
      "Requirement already satisfied: torchsummary in /data/hien/.local/lib/python3.13/site-packages (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in /data/hien/.local/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (7.0.0)\n",
      "Requirement already satisfied: networkx in /data/hien/.local/lib/python3.13/site-packages (from obonet) (3.6)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.13/site-packages (from pyvis) (9.1.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.13/site-packages (from pyvis) (3.1.6)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /data/hien/.local/lib/python3.13/site-packages (from pyvis) (4.1.1)\n",
      "Requirement already satisfied: filelock in /data/hien/.local/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /data/hien/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /data/hien/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /data/hien/.local/lib/python3.13/site-packages (from torchmetrics) (2.9.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /data/hien/.local/lib/python3.13/site-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.13/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2>=2.9.6->pyvis) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (78.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.13/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/hien/.local/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!./create-rapids-env.sh {BASE_PATH}\n",
    "!./create-pytorch-env.sh {BASE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Get the input data\n",
    "\n",
    "Here we describe what should be stored in the working dir to reproduce the results\n",
    "\n",
    "Following data scheme was provided by Kaggle:\n",
    "\n",
    "    ./Train - cafa train data\n",
    "    ./Test - cafa test data\n",
    "    ./sample_submission.tsv - cafa ssub\n",
    "    ./IA.txt - cafa IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data\n",
    "!{RAPIDS_ENV} standard_data/standard.py\n",
    "!{RAPIDS_ENV} standard_data/check_ia.py\n",
    "# If you want to use taxonomy features, add this to preprocess.py\n",
    "!{RAPIDS_ENV} standard_data/get_tax_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the solution code libraries, scipts, and notebooks used for training:\n",
    "\n",
    "    ./protlib\n",
    "    ./protnn\n",
    "    ./nn_solution\n",
    "    \n",
    "And the installed envs\n",
    "\n",
    "    ./pytorch-env\n",
    "    ./rapids-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Produce the helpers data\n",
    "\n",
    "First, we made some preprocessing of the input data to store everything in format that is convinient to us to handle and manipulate. Here is the structure:\n",
    "\n",
    "    ./helpers\n",
    "        ./fasta - fasta files stored as feather\n",
    "            ./train_seq.feather\n",
    "            ./test_seq.feather\n",
    "        ./real_targets - targets stored as n_proteins x n_terms parquet containing 0/1/NaN values\n",
    "            ./biological_process\n",
    "                ./part_0.parquet\n",
    "                ...\n",
    "                ./part_14.parquet\n",
    "                ./nulls.pkl - NaN rate of each term\n",
    "                ./priors.pkl - prior mean of each term (excluding NaN cells, like np.nanmean)\n",
    "            ./cellular_component\n",
    "            ./molecular_function\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845244it [00:00, 1013180.07it/s]\n",
      "1941130it [00:00, 2044347.95it/s]\n",
      "/data/hien/CAFA-6/U900\n",
      "Propagate:  True\n",
      "2it [09:49, 294.53s/it]\n",
      "CPU times: user 3.45 s, sys: 690 ms, total: 4.14 s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parse fasta files and save as feather\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_fasta.py \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "# convert targets to parquet and calculate priors\n",
    "!{RAPIDS_ENV} protlib/scripts/create_helpers.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --batch-size 40000 \\\n",
    "    --propagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Get external data\n",
    "\n",
    "Datasets downloaded from outside and then processed. First step is downloading and parsing the datasets. After parsing, script will separate the datasets by the evidence codes. The most important split for us is kaggle/no-kaggle split. We refer `kaggle` as experimental codes, `no-kaggle` as electornic labeling, that will be used as features for the stacker models. Downloading takes quite a long time, while processing takes about 1 hour. The required structure after execution\n",
    "\n",
    "    ./temporal - extra data downloaded from http://ftp.ebi.ac.uk/pub/databases/GO/goa/old/UNIPROT/\n",
    "    ./labels   - extracted and propagated labeling\n",
    "        ./prop_test_leak_no_dup.tsv - labels from newest version\n",
    "        ./prop_test_no_kaggle.tsv   - electronic labels test\n",
    "        ./prop_train_no_kaggle.tsv  - electronic labels train\n",
    "        \n",
    "    ./cafa-terms-diff.tsv - different labels from between 2 recent versions.\n",
    "    ./prop_quickgo51.tsv  - labels from old version\n",
    "    \n",
    "    \n",
    "Other files are temporary and not needed for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, **you should run this on terminal** with **tmux**, with default 16 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download external data from ebi.ac.uk\n",
    "!{RAPIDS_ENV} protlib/scripts/downloads/dw_goant.py --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270it [05:14,  1.18s/it]^C\n",
      "270it [05:14,  1.17s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/scripts/parse_go_single.py\", line 52, in <module>\n",
      "    for n, batch in tqdm.tqdm(enumerate(reader)):\n",
      "  File \"/data/hien/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1843, in __next__\n",
      "    return self.get_chunk()\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1985, in get_chunk\n",
      "    return self.read(nrows=size)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "  File \"parsers.pyx\", line 850, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
      "  File \"parsers.pyx\", line 2061, in pandas._libs.parsers.raise_parser_error\n",
      "pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "2it [00:02,  1.26s/it]^C\n",
      "2it [00:03,  1.73s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/scripts/parse_go_single.py\", line 52, in <module>\n",
      "    for n, batch in tqdm.tqdm(enumerate(reader)):\n",
      "  File \"/data/hien/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1843, in __next__\n",
      "    return self.get_chunk()\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1985, in get_chunk\n",
      "    return self.read(nrows=size)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "  File \"parsers.pyx\", line 850, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
      "  File \"parsers.pyx\", line 2061, in pandas._libs.parsers.raise_parser_error\n",
      "pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n"
     ]
    }
   ],
   "source": [
    "# # parse the files\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.226.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output ver226\n",
    "\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.228.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output ver228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is propagation. Since ebi.ac datasets contains the labeling without propagation, we will apply the rules provided in organizer's repo to labeling more terms. We will do it only for `goa_uniprot_all.gaf.228.gz` datasets since it is the actual dataset at the active competition phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "folder = BASE_PATH + '/temporal/ver228'\n",
    "\n",
    "for file in glob.glob(folder + '/labels/train*') + glob.glob(folder + '/labels/test*'):\n",
    "    name = folder + '/labels/prop_' + file.split('/')[-1]\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "        --path {file} \\\n",
    "        --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "        --output {name} \\\n",
    "        --device 0 \\\n",
    "        --batch_size 30000 \\\n",
    "        --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `cafa-terms-diff` dataset, that represents the difference between our labeling obtained by parsing `goa_uniprot_all.gaf.228.gz` dataset and `goa_uniprot_all.gaf.226.gz`. So the difference is actually the temporal. We removed duplicated protein/terms pairs from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hien/CAFA-6/U900\n",
      "^C\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py\", line 40, in <module>\n",
      "    import cupy as cp\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupy/__init__.py\", line 30, in <module>\n",
      "    import cupyx as _cupyx  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/__init__.py\", line 8, in <module>\n",
      "    from cupyx import linalg  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/linalg/__init__.py\", line 2, in <module>\n",
      "    from cupyx.linalg import sparse  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/linalg/sparse/__init__.py\", line 3, in <module>\n",
      "    from cupyx.linalg.sparse._solve import lschol  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/linalg/sparse/_solve.py\", line 6, in <module>\n",
      "    from cupyx.scipy import sparse\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/scipy/__init__.py\", line 4, in <module>\n",
      "    from cupyx.scipy.sparse._base import spmatrix as _spmatrix\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/scipy/sparse/__init__.py\", line 1, in <module>\n",
      "    from cupyx.scipy.sparse._base import issparse  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/scipy/sparse/_base.py\", line 10, in <module>\n",
      "    import scipy.sparse as _sparse\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/sparse/__init__.py\", line 300, in <module>\n",
      "    from ._base import *\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/sparse/_base.py\", line 5, in <module>\n",
      "    from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/sparse/_sputils.py\", line 10, in <module>\n",
      "    from scipy._lib._util import np_long, np_ulong\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/_lib/_util.py\", line 13, in <module>\n",
      "    from scipy._lib._array_api import array_namespace, is_numpy, xp_size\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/_lib/_array_api.py\", line 18, in <module>\n",
      "    from scipy._lib.array_api_compat import (\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py\", line 1, in <module>\n",
      "    from numpy import * # noqa: F403\n",
      "  File \"<frozen importlib._bootstrap>\", line 1073, in _handle_fromlist\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/__init__.py\", line 361, in __getattr__\n",
      "    import numpy.f2py as f2py\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/f2py/__init__.py\", line 18, in <module>\n",
      "    from . import f2py2e\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/f2py/f2py2e.py\", line 19, in <module>\n",
      "    from . import crackfortran\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/f2py/crackfortran.py\", line 3185, in <module>\n",
      "    determineexprtype_re_2 = re.compile(r'\\A[+-]?\\d+(_(?P<name>\\w+)|)\\Z', re.I)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/re.py\", line 251, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/re.py\", line 303, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 792, in compile\n",
      "    code = _code(p, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 631, in _code\n",
      "    _compile(code, p.data, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 164, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 146, in _compile\n",
      "    _compile_charset(charset, flags, code)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 270, in _compile_charset\n",
      "    if op is NEGATE:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/reproduce_mt.py \\\n",
    "    --path {BASE_PATH}/temporal/ver228 \\\n",
    "    --old-path {BASE_PATH}/temporal/ver226 \\\n",
    "    --new-path {BASE_PATH}/temporal/ver228 \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --target {BASE_PATH}/embeds/esm_small/train_ids.npy\n",
    "\n",
    "# # make propagation for quickgo51.tsv\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "    --path {BASE_PATH}/temporal/ver228/quickgo51.tsv \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --output {BASE_PATH}/temporal/ver228/prop_quickgo51.tsv \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Preparation step for neural networks\n",
    "\n",
    "Produce some helpers to train NN model. Creates the following data:\n",
    "\n",
    "    ./helpers/feats\n",
    "        ./train_ids_cut43k.npy\n",
    "        ./Y_31466_labels.npy\n",
    "        ./Y_31466_sparse_float32.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537027, 3)\n",
      "term\n",
      "GO:0005515    33713\n",
      "GO:0005634    13283\n",
      "GO:0005829    13040\n",
      "GO:0005886    10150\n",
      "GO:0005737     9442\n",
      "              ...  \n",
      "GO:0010622        1\n",
      "GO:0042357        1\n",
      "GO:0009421        1\n",
      "GO:0008764        1\n",
      "GO:0061336        1\n",
      "Name: count, Length: 26125, dtype: int64\n",
      "aspect\n",
      "BPO    16858\n",
      "CCO     2651\n",
      "MFO     6616\n",
      "Name: term, dtype: int64\n",
      "CPU times: user 8.97 ms, sys: 9 ms, total: 18 ms\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/nn_solution/prepare.py \\\n",
    "    --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Embeddings from parent folder of this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 T5 pretrained inference\n",
    "\n",
    "    ./embeds\n",
    "        ./t5\n",
    "            ./train_ids.npy\n",
    "            ./train_embeds.npy\n",
    "            ./test_ids.npy\n",
    "            ./test_embeds.npy\n",
    "\n",
    "### 2.2 ESM pretrained inference\n",
    "\n",
    "    ./embeds\n",
    "        ./esm_small\n",
    "            ./train_ids.npy\n",
    "            ./train_embeds.npy\n",
    "            ./test_ids.npy\n",
    "            ./test_embeds.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train and inference py-boost models\n",
    "\n",
    "GBDT models description:\n",
    "\n",
    "1) Features: T5 + taxon, targets: multilabel\n",
    "\n",
    "2) Features: T5 + taxon, targets: conditional\n",
    "\n",
    "3) Features: T5 + ESM + taxon, targets: multilabel\n",
    "\n",
    "4) Features: T5 + ESM + taxon, targets: conditional\n",
    "\n",
    "Pipeline and hyperparameters are the same for all the models. Target is 4500 output: BP 3000, MF: 1000, CC: 500. All models could be ran in parallel to save a time.\n",
    "    \n",
    "    ./models\n",
    "        ./pb_t54500_raw\n",
    "            ./models_0.pkl\n",
    "            ...\n",
    "            ./models_4.pkl\n",
    "            ./oof_pred.pkl\n",
    "            ./test_pred.pkl\n",
    "        ./pb_t54500_cond\n",
    "            ...\n",
    "        ./pb_t5esm4500_raw\n",
    "            ...\n",
    "        ./pb_t5esm4500_cond\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train and inference logreg models\n",
    "\n",
    "Logistic Regression models description:\n",
    "\n",
    "1) Features: T5 + taxon, targets: multilabel\n",
    "\n",
    "2) Features: T5 + taxon, targets: conditional\n",
    "\n",
    "\n",
    "Pipeline and hyperparameters are the same for all the models. Target is 13500 output: BP 10000, MF: 2000, CC: 1500. All models could be ran in parallel to save a time.\n",
    "\n",
    "    ./helpers\n",
    "        ./folds_gkf.npy\n",
    "    ./models\n",
    "        ./lin_t5_raw\n",
    "            ./models_0.pkl\n",
    "            ...\n",
    "            ./models_4.pkl\n",
    "            ./oof_pred.pkl\n",
    "            ./test_pred.pkl\n",
    "        ./lin_t5_cond\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train and inference NN models\n",
    "\n",
    "Structure is:\n",
    "\n",
    "    ./models\n",
    "        ./nn_serg\n",
    "            ./model_0_0.pt\n",
    "            ...\n",
    "            ./model_11_4.pt\n",
    "            ./pytorch-keras-etc-3-blend-cafa-metric-etc.pkl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Train all models parallel on 8 x A100 80GB\n",
    "We use all GPU and cost 2 hours to train all models.\n",
    "Estimate about 5 days with a single V100 32GB. But remember to custom the setting inside the script file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "!{PYTORCH_ENV} {BASE_PATH}/parallel/train_all_models_fold.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final model\n",
    "\n",
    "### 4.1. Train GCN models\n",
    "\n",
    "This step is training 3 independent stacking models for each ontology. Models are trained on 3xA100 cost about 4 hours, but estimate about 13 hours for BP, 4 hours for MF and 2 hours for CC on single V100 GPU. \n",
    "\n",
    "    ./models\n",
    "        ./gcn\n",
    "            ./bp\n",
    "                ./checkpoint.pth\n",
    "            ./mf\n",
    "                ./checkpoint.pth\n",
    "            ./cc\n",
    "                ./checkpoint.pth\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/parallel/train_gcn_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inference GCN models and TTA\n",
    "\n",
    "Inference and Test-Time-Augmentation. Structure:\n",
    "\n",
    "    ./models\n",
    "        ./gcn\n",
    "            ./pred_tta_0.tsv\n",
    "            ...\n",
    "            ./pred_tta_3.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['./models/pb_t54500_cond', [3000, 1000, 500], True], ['./models/pb_t54500_raw', [3000, 1000, 500], False], ['./models/lin_t5_cond', [10000, 2000, 1500], True], ['./models/lin_t5_raw', [10000, 2000, 1500], False]]\n",
      "  0%|                                                  | 0/1753 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/protnn/stacker.py:17: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/Indexing.cu:1454.)\n",
      "  x0.index_reduce(1, dst, x0[:, src], reduce='mean', include_self=False),\n",
      "100%|███████████████████████████████████████| 1753/1753 [23:34<00:00,  1.24it/s]\n",
      "[['./models/pb_t5esm4500_cond', [3000, 1000, 500], True], ['./models/pb_t54500_raw', [3000, 1000, 500], False], ['./models/lin_t5_cond', [10000, 2000, 1500], True], ['./models/lin_t5_raw', [10000, 2000, 1500], False]]\n",
      " 74%|████████████████████████████▉          | 1300/1753 [17:45<06:03,  1.25it/s]^C\n",
      "CPU times: user 14.8 s, sys: 3.35 s, total: 18.1 s\n",
      "Wall time: 42min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!{PYTORCH_ENV} {BASE_PATH}/parallel/predict_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Postprocessing and build submission file\n",
    "\n",
    "Here we do the following:\n",
    "\n",
    "1) Average TTA predictions\n",
    "2) Perform min prop\n",
    "3) Perform max prop\n",
    "4) Average min/max prop steps, add external leakage data and make submission\n",
    "\n",
    "Structure:\n",
    "\n",
    "    ./models\n",
    "        ./postproc\n",
    "            ./pred.tsv     - avg TTA\n",
    "            ./pred_min.tsv - min prop\n",
    "            ./pred_max.tsv - max prop\n",
    "            \n",
    "    ./sub\n",
    "        ./submission.tsv   - final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 0           O14788\n",
      "1           O14788\n",
      "2           O14788\n",
      "3           O14788\n",
      "4           O14788\n",
      "             ...  \n",
      "38553236    P38306\n",
      "38553237    P36951\n",
      "38557695    Q9C0W5\n",
      "38557696    Q9HDZ0\n",
      "38557697    Q9P7M4\n",
      "Name: entry_id, Length: 27046121, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A098D1J7        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4IB93        2\n",
      "A0A1D8PDP8        3\n",
      "A0A1W2P7I0        4\n",
      "              ...  \n",
      "Q9ZW22        79026\n",
      "Q9ZW81        79027\n",
      "R9QMR2        79028\n",
      "T2HG31        79029\n",
      "W8JIS5        79030\n",
      "Name: index, Length: 79031, dtype: int64\n",
      " - Tổng số dòng dự đoán: 27046121\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:16, 16.84s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:29, 14.33s/it]                            | 1/3 [00:13<00:26, 13.40s/it]\u001b[A\n",
      "3it [00:41, 13.36s/it]█████████████               | 2/3 [00:25<00:12, 12.91s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:38<00:00, 12.73s/it]\u001b[A\n",
      "3it [00:41, 13.91s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 26045044    A2A2Y4\n",
      "26045045    A2A2Y4\n",
      "26045046    A2A2Y4\n",
      "26045047    A2A2Y4\n",
      "26045048    A2A2Y4\n",
      "             ...  \n",
      "38561935    Q12179\n",
      "38561936    Q12259\n",
      "38561937    Q12259\n",
      "38561938    Q12486\n",
      "38561939    Q12134\n",
      "Name: entry_id, Length: 6596695, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0G2KBC9        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4INB9        2\n",
      "A0A1P8BH59        3\n",
      "A0A3F2YLV2        4\n",
      "              ...  \n",
      "Q9ZW81        78906\n",
      "Q9ZWJ3        78907\n",
      "R9QMR2        78908\n",
      "T2HG31        78909\n",
      "W8JIS5        78910\n",
      "Name: index, Length: 78911, dtype: int64\n",
      " - Tổng số dòng dự đoán: 6596695\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:10, 10.16s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:17,  8.22s/it]                            | 1/3 [00:06<00:13,  6.79s/it]\u001b[A\n",
      "3it [00:23,  7.35s/it]█████████████               | 2/3 [00:13<00:06,  6.83s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:19<00:00,  6.66s/it]\u001b[A\n",
      "3it [00:23,  7.83s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 32546490    B3KU38\n",
      "32546491    B3KU38\n",
      "32546492    B3KU38\n",
      "32546493    B3KU38\n",
      "32546494    B3KU38\n",
      "             ...  \n",
      "38735398    P37683\n",
      "38735399    P40052\n",
      "38735400    P31437\n",
      "38735401    P39270\n",
      "38735402    P39721\n",
      "Name: entry_id, Length: 5067582, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0P0XII1        0\n",
      "A0A1D8PDP8        1\n",
      "A0A286ZK88        2\n",
      "A0A287B8J2        3\n",
      "A0A2R8Y4L2        4\n",
      "              ...  \n",
      "Q9ZW81        79041\n",
      "Q9ZWJ3        79042\n",
      "R9QMR2        79043\n",
      "T2HG31        79044\n",
      "W8JIS5        79045\n",
      "Name: index, Length: 79046, dtype: int64\n",
      " - Tổng số dòng dự đoán: 5067582\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:09,  9.08s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:14,  7.15s/it]                            | 1/3 [00:06<00:12,  6.18s/it]\u001b[A\n",
      "3it [00:21,  6.74s/it]█████████████               | 2/3 [00:11<00:05,  5.96s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:18<00:00,  6.08s/it]\u001b[A\n",
      "3it [00:21,  7.08s/it]\n",
      "CAFA5 Scores\n",
      "{'bp': 0.7601713738902395, 'mf': 0.8485638329405198, 'cc': 0.8516544708841466, 'cafa': np.float64(0.8201298925716354)}\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 12%|█████▋                                       | 1/8 [01:07<07:55, 67.93s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 25%|███████████▎                                 | 2/8 [02:06<06:15, 62.63s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 38%|████████████████▉                            | 3/8 [02:59<04:50, 58.03s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 50%|██████████████████████▌                      | 4/8 [03:48<03:38, 54.63s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 62%|████████████████████████████▏                | 5/8 [04:34<02:34, 51.54s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 75%|█████████████████████████████████▊           | 6/8 [05:21<01:39, 49.71s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [06:00<00:46, 46.43s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "100%|█████████████████████████████████████████████| 8/8 [06:23<00:00, 47.92s/it]\n",
      "  0%|                                                     | 0/8 [00:00<?, ?it/s]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 12%|█████▋                                       | 1/8 [00:57<06:40, 57.21s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 25%|███████████▎                                 | 2/8 [01:48<05:21, 53.64s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 38%|████████████████▉                            | 3/8 [02:31<04:03, 48.74s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 50%|██████████████████████▌                      | 4/8 [03:15<03:07, 46.83s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 62%|████████████████████████████▏                | 5/8 [03:53<02:11, 43.88s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 75%|█████████████████████████████████▊           | 6/8 [04:29<01:22, 41.23s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [05:02<00:38, 38.35s/it]/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "/data/hien/CAFA-6/U900/.//protlib/scripts/postproc/step.py:126: DeprecationWarning: `ndarray.scatter_add` is deprecated. Please use `cupy.add.at` instead.\n",
      "  mat.scatter_add((sample_ont['id'].values, sample_ont['term_id'].values), sample_ont['prob'].values)\n",
      "100%|█████████████████████████████████████████████| 8/8 [05:18<00:00, 39.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# since we have 4 TTA predictions, we need to aggregate all as an average\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/collect_ttas.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0\n",
    "\n",
    "# create 0.3 * pred + 0.7 * max children propagation\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/step.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 3000 \\\n",
    "    --lr 0.7 \\\n",
    "    --direction min\n",
    "\n",
    "# create 0.3 * pred + 0.7 * min parents propagation\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/step.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 3000 \\\n",
    "    --lr 0.7 \\\n",
    "    --direction max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 1           Q9SZP1\n",
      "3           Q9JLQ2\n",
      "4           P26232\n",
      "5           Q96CV9\n",
      "7           Q07303\n",
      "             ...  \n",
      "41661717    Q15154\n",
      "41661718    P21744\n",
      "41661719    Q94FV7\n",
      "41661720    Q6T3U4\n",
      "41661721    O55098\n",
      "Name: entry_id, Length: 29205787, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A098D1J7        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4IB93        2\n",
      "A0A1D8PDP8        3\n",
      "A0A1W2P7I0        4\n",
      "              ...  \n",
      "Q9ZW22        79026\n",
      "Q9ZW81        79027\n",
      "R9QMR2        79028\n",
      "T2HG31        79029\n",
      "W8JIS5        79030\n",
      "Name: index, Length: 79031, dtype: int64\n",
      " - Tổng số dòng dự đoán: 29205787\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:14, 14.89s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:26, 12.95s/it]                            | 1/3 [00:11<00:23, 11.67s/it]\u001b[A\n",
      "3it [00:38, 12.31s/it]█████████████               | 2/3 [00:23<00:11, 11.62s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:34<00:00, 11.61s/it]\u001b[A\n",
      "3it [00:38, 12.68s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 13          P53366\n",
      "14          Q925U2\n",
      "34          Q7F1M0\n",
      "37          P15442\n",
      "41          Q55AR3\n",
      "             ...  \n",
      "41608768    P15387\n",
      "41608775    Q12358\n",
      "41608777    O43736\n",
      "41608780    P05508\n",
      "41608782    P50942\n",
      "Name: entry_id, Length: 7064878, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0G2KBC9        0\n",
      "A0A0P0XII1        1\n",
      "A0A0R4INB9        2\n",
      "A0A1P8BH59        3\n",
      "A0A3F2YLV2        4\n",
      "              ...  \n",
      "Q9ZW81        78906\n",
      "Q9ZWJ3        78907\n",
      "R9QMR2        78908\n",
      "T2HG31        78909\n",
      "W8JIS5        78910\n",
      "Name: index, Length: 78911, dtype: int64\n",
      " - Tổng số dòng dự đoán: 7064878\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:18, 18.58s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:24, 11.21s/it]                            | 1/3 [00:15<00:31, 15.94s/it]\u001b[A\n",
      "3it [00:30,  8.72s/it]█████████████               | 2/3 [00:21<00:10, 10.13s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:27<00:00,  9.25s/it]\u001b[A\n",
      "3it [00:30, 10.17s/it]\n",
      "0it [00:00, ?it/s]\n",
      "[DEBUG] Bắt đầu kiểm tra mapping ID:\n",
      " - Tổng số protein trong file dự đoán (sub.tsv): 5               P14575\n",
      "9               P32583\n",
      "16              Q43315\n",
      "20              P51654\n",
      "23              P23763\n",
      "               ...    \n",
      "41784599        P17152\n",
      "41784613        Q95XU6\n",
      "41784619        Q9MAC1\n",
      "41784625    A0A1D8PJX3\n",
      "41784647        Q8TBC4\n",
      "Name: entry_id, Length: 5513334, dtype: object\n",
      " - Tổng số protein có trong nhãn (validation): entry_id\n",
      "A0A0P0XII1        0\n",
      "A0A1D8PDP8        1\n",
      "A0A286ZK88        2\n",
      "A0A287B8J2        3\n",
      "A0A2R8Y4L2        4\n",
      "              ...  \n",
      "Q9ZW81        79041\n",
      "Q9ZWJ3        79042\n",
      "R9QMR2        79043\n",
      "T2HG31        79044\n",
      "W8JIS5        79045\n",
      "Name: index, Length: 79046, dtype: int64\n",
      " - Tổng số dòng dự đoán: 5513334\n",
      " - Số dòng bị NaN (không khớp ID): 0\n",
      "\n",
      "1it [00:07,  7.71s/it]                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "2it [00:13,  6.33s/it]                            | 1/3 [00:05<00:10,  5.17s/it]\u001b[A\n",
      "3it [00:17,  5.63s/it]█████████████               | 2/3 [00:10<00:05,  5.29s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:15<00:00,  5.11s/it]\u001b[A\n",
      "3it [00:18,  6.02s/it]\n",
      "CAFA5 Scores\n",
      "{'bp': 0.9926301802846386, 'mf': 0.9775300280161725, 'cc': 0.9943924744303683, 'cafa': np.float64(0.9881842275770598)}\n"
     ]
    }
   ],
   "source": [
    "# here we average min prop and max prop solutions, mix with cafa-terms-diff and quickgo51 datasets from 1.4\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/make_submission.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --device 0 \\\n",
    "    --max-rate 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading main submission: sub/submission-308.tsv\n",
      "Loading GOA leak...\n",
      "Loading QuickGO51...\n",
      "Loading Diff terms...\n",
      "Concatenating all sources...\n",
      "Parsing OBO graph for namespaces...\n",
      "Saving final submission to: ./sub/submission.tsv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# # here we average min prop and max prop solutions, mix with cafa-terms-diff and quickgo51 datasets from 1.4\n",
    "# !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/make_submission2.py \\\n",
    "#     --config-path {CONFIG_PATH} \\\n",
    "#     --input-file \"sub/submission-308.tsv\" \\\n",
    "#     --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "Result is stored in `./sub/submission.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P62500\tGO:0030217\t0.0265\n",
      "Q06449\tGO:0005215\t0.015\n",
      "Q9M4C1\tGO:0043229\t0.961\n",
      "P59368\tGO:0017080\t0.549\n",
      "Q9Y7B3\tGO:0045892\t0.016\n",
      "Q9FMQ1\tGO:0009416\t0.0185\n",
      "P42335\tGO:0040014\t0.004\n",
      "Q9USW8\tGO:0051130\t0.0115\n",
      "P04759\tGO:0038023\t0.91425\n",
      "Q2SX36\tGO:0009058\t0.4485\n"
     ]
    }
   ],
   "source": [
    "!head {BASE_PATH}/sub/submission.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cafa)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
