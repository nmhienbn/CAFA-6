{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware requirements\n",
    "- GPU: 8 x A100 80GB\n",
    "- CPU: AMD 2TB, 256 cores\n",
    "- Disk Storage: 2TB (At least 300GB for this project)\n",
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_path: ./ # working dir\n",
      "cafa_data_path: ../data/raw/cafa6 # working dir\n",
      "# environments\n",
      "rapids-env: rapids-env/bin/python\n",
      "pytorch-env: pytorch-env/bin/python\n",
      "# artifacts paths\n",
      "embeds_path: ../features/embeds # path to embeddings \n",
      "models_path: ../models # store the models\n",
      "helpers_path: ../features/helpers # store reformated datasets\n",
      "temporal_path: ../features/temporal # store external data from FTP (temporal because different report dates are used)\n",
      "\n",
      "\n",
      "base_models: # all models and postprocessing path\n",
      "    pb_t5esm4500_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "            - esm_small\n",
      "        conditional: false\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t5esm4500_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "            - esm_small\n",
      "        conditional: true\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t54500_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: false\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    pb_t54500_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: true\n",
      "        bp: 3000\n",
      "        mf: 1000\n",
      "        cc: 500\n",
      "        \n",
      "    lin_t5_raw:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: false\n",
      "        bp: 10000\n",
      "        mf: 2000\n",
      "        cc: 1500\n",
      "        \n",
      "    lin_t5_cond:\n",
      "        embeds: \n",
      "            - t5\n",
      "        conditional: true\n",
      "        bp: 10000\n",
      "        mf: 2000\n",
      "        cc: 1500\n",
      "        \n",
      "public_models: # models based on public script\n",
      "    nn_serg:\n",
      "        source: pytorch-keras-etc-3-blend-cafa-metric-etc.pkl\n",
      "        # source: test_nn.pkl\n",
      "        \n",
      "gcn: # stacking with graph neural network - separated by ontology\n",
      "    bp:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "\n",
      "    mf:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "                \n",
      "    cc:\n",
      "        n_ep: 20\n",
      "        store_swa: 10\n",
      "        use_swa: 3\n",
      "        \n",
      "        hidden_size: 16\n",
      "        n_layers: 8\n",
      "        embed_size: 8\n",
      "        \n",
      "        preds:\n",
      "            - pb_t54500_cond\n",
      "            - pb_t54500_raw\n",
      "            - lin_t5_cond\n",
      "            - lin_t5_raw\n",
      "            \n",
      "        side_preds:\n",
      "            - nn_serg\n",
      "            \n",
      "        tta:\n",
      "            cfg0:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg1:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t54500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg2:\n",
      "                - pb_t54500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n",
      "            cfg3:\n",
      "                - pb_t5esm4500_cond\n",
      "                - pb_t5esm4500_raw\n",
      "                - lin_t5_cond\n",
      "                - lin_t5_raw\n"
     ]
    }
   ],
   "source": [
    "!cat config.yaml\n",
    "\n",
    "with open('config.yaml') as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "    \n",
    "BASE_PATH = CONFIG['base_path']\n",
    "CONFIG_PATH = os.path.join(BASE_PATH, 'config.yaml')\n",
    "RAPIDS_ENV = os.path.join(BASE_PATH, CONFIG['rapids-env'])\n",
    "PYTORCH_ENV = os.path.join(BASE_PATH, CONFIG['pytorch-env'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "### 1.1. Setup envs\n",
    "\n",
    "Create the following python envs:\n",
    "\n",
    "* `pytorch-env` - env to deal with all DL models\n",
    "* `rapids-env`  - env to preprocess via RAPIDS and train py-boost and logregs\n",
    "\n",
    "**You should run this outside terminal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - rapidsai\n",
      " - conda-forge\n",
      " - nvidia\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/hien/CAFA-6/U900/rapids-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - cuda-version=11.8\n",
      "    - cudatoolkit=11.8\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  cuda-version       conda-forge/noarch::cuda-version-11.8-h70ddcb2_3 \n",
      "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.8.0-h4ba93d1_13 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_16 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_16 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_16 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libstdcxx          conda-forge/linux-64::libstdcxx-15.2.0-h934c35e_16 \n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-15.2.0-hdf11a46_16 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-h5347b49_1 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.3-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.10.19-h3c07f61_2_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: \\ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /data/hien/CAFA-6/U900/rapids-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting cupy-cuda11x\n",
      "  Downloading cupy_cuda11x-13.6.0-cp313-cp313-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting numba==0.56.4\n",
      "  Downloading numba-0.56.4.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[26 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-sj7c6pmf/numba_9f477b62ec86466c84488e9b76df48be/versioneer.py:415: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \u001b[31m   \u001b[0m   mo = re.search(r'=\\s*\"(.*)\"', line)\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-o4z0s_o6/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m51\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m_guard_py_ver\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCannot install on Python version 3.13.5; only versions >=3.7,<3.11 are supported.\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'numba' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting cudf-cu11\n",
      "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuml-cu11\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cugraph-cu11\n",
      "  Downloading https://pypi.nvidia.com/cugraph-cu11/cugraph_cu11-25.6.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools (from cudf-cu11)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting cubinlinker-cu11 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuda-python<12.0a0,>=11.8.5 (from cudf-cu11)\n",
      "  Using cached cuda_python-11.8.7-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cupy-cuda11x>=12.0.0 (from cudf-cu11)\n",
      "  Using cached cupy_cuda11x-13.6.0-cp313-cp313-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /data/hien/.local/lib/python3.13/site-packages (from cudf-cu11) (2025.10.0)\n",
      "Collecting libcudf-cu11==25.6.* (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/libcudf-cu11/libcudf_cu11-25.6.0-py3-none-manylinux_2_28_x86_64.whl (445.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.9/445.9 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numba-cuda<0.12.0a0,>=0.11.0 (from cudf-cu11)\n",
      "  Using cached numba_cuda-0.11.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting numba<0.62.0a0,>=0.59.1 (from cudf-cu11)\n",
      "  Downloading numba-0.61.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /data/hien/.local/lib/python3.13/site-packages (from cudf-cu11) (2.3.5)\n",
      "Collecting nvtx>=0.2.1 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/nvtx/nvtx-0.2.14-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (717 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.8/717.8 kB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.13/site-packages (from cudf-cu11) (25.0)\n",
      "Collecting pandas<2.2.4dev0,>=2.0 (from cudf-cu11)\n",
      "  Downloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting ptxcompiler-cu11 (from cudf-cu11)\n",
      "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.8.1.post3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m6.6/8.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "/data/hien/CAFA-6/U900\n",
      "conda 25.7.0\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.7.0\n",
      "    latest version: 25.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /data/hien/CAFA-6/U900/pytorch-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.9\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-hda65f42_8 \n",
      "  ca-certificates    conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.45-default_hbd61a6d_104 \n",
      "  libexpat           conda-forge/linux-64::libexpat-2.7.3-hecca717_0 \n",
      "  libffi             conda-forge/linux-64::libffi-3.5.2-h9ec8514_0 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-he0feb66_16 \n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.2.0-h69a702a_16 \n",
      "  libgomp            conda-forge/linux-64::libgomp-15.2.0-he0feb66_16 \n",
      "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
      "  libsqlite          conda-forge/linux-64::libsqlite-3.51.1-h0c1763c_0 \n",
      "  libuuid            conda-forge/linux-64::libuuid-2.41.2-h5347b49_1 \n",
      "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
      "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
      "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
      "  openssl            conda-forge/linux-64::openssl-3.6.0-h26f9b46_0 \n",
      "  pip                conda-forge/noarch::pip-25.2-pyh8b19718_0 \n",
      "  python             conda-forge/linux-64::python-3.9.23-hc30ae73_0_cpython \n",
      "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
      "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
      "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_ha0e22de_103 \n",
      "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
      "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb78ec9c_6 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /data/hien/CAFA-6/U900/pytorch-env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 1002\n",
      "  gid: 1002\n",
      "\n",
      "\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /opt/conda\n",
      "  uid: 1002\n",
      "  gid: 1002\n",
      "\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /data/hien/.local/lib/python3.13/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5.tar.gz (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.13/site-packages (6.0.3)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting numba==0.57.1\n",
      "  Downloading numba-0.57.1.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[24 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/data/hien/.local/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/tmp/pip-build-env-g2_alglw/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m51\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m48\u001b[0m, in \u001b[35m_guard_py_ver\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCannot install on Python version 3.13.5; only versions >=3.8,<3.12 are supported.\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'numba' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: obonet in /data/hien/.local/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: pyvis in /data/hien/.local/lib/python3.13/site-packages (0.3.2)\n",
      "Requirement already satisfied: transformers in /data/hien/.local/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torchmetrics in /data/hien/.local/lib/python3.13/site-packages (1.8.2)\n",
      "Requirement already satisfied: torchsummary in /data/hien/.local/lib/python3.13/site-packages (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in /data/hien/.local/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.13/site-packages (7.0.0)\n",
      "Requirement already satisfied: networkx in /data/hien/.local/lib/python3.13/site-packages (from obonet) (3.6)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.13/site-packages (from pyvis) (9.1.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.13/site-packages (from pyvis) (3.1.6)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /data/hien/.local/lib/python3.13/site-packages (from pyvis) (4.1.1)\n",
      "Requirement already satisfied: filelock in /data/hien/.local/lib/python3.13/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /data/hien/.local/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /data/hien/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /data/hien/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /data/hien/.local/lib/python3.13/site-packages (from torchmetrics) (2.9.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /data/hien/.local/lib/python3.13/site-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.13/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.13/site-packages (from jinja2>=2.9.6->pyvis) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.13/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (78.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.13/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /data/hien/.local/lib/python3.13/site-packages (from torch>=2.0.0->torchmetrics) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/hien/.local/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.13/site-packages (from stack_data->ipython>=5.3.0->pyvis) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!./create-rapids-env.sh {BASE_PATH}\n",
    "!./create-pytorch-env.sh {BASE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Get the input data\n",
    "\n",
    "Here we describe what should be stored in the working dir to reproduce the results\n",
    "\n",
    "Following data scheme was provided by Kaggle:\n",
    "\n",
    "    ./Train - cafa train data\n",
    "    ./Test - cafa test data\n",
    "    ./sample_submission.tsv - cafa ssub\n",
    "    ./IA.txt - cafa IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data\n",
    "!{RAPIDS_ENV} standard_data/standard.py\n",
    "!{RAPIDS_ENV} standard_data/check_ia.py\n",
    "# If you want to use taxonomy features, add this to preprocess.py\n",
    "!{RAPIDS_ENV} standard_data/get_tax_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the solution code libraries, scipts, and notebooks used for training:\n",
    "\n",
    "    ./protlib\n",
    "    ./protnn\n",
    "    ./nn_solution\n",
    "    \n",
    "And the installed envs\n",
    "\n",
    "    ./pytorch-env\n",
    "    ./rapids-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Produce the helpers data\n",
    "\n",
    "First, we made some preprocessing of the input data to store everything in format that is convinient to us to handle and manipulate. Here is the structure:\n",
    "\n",
    "    ./helpers\n",
    "        ./fasta - fasta files stored as feather\n",
    "            ./train_seq.feather\n",
    "            ./test_seq.feather\n",
    "        ./real_targets - targets stored as n_proteins x n_terms parquet containing 0/1/NaN values\n",
    "            ./biological_process\n",
    "                ./part_0.parquet\n",
    "                ...\n",
    "                ./part_14.parquet\n",
    "                ./nulls.pkl - NaN rate of each term\n",
    "                ./priors.pkl - prior mean of each term (excluding NaN cells, like np.nanmean)\n",
    "            ./cellular_component\n",
    "            ./molecular_function\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845244it [00:00, 1013180.07it/s]\n",
      "1941130it [00:00, 2044347.95it/s]\n",
      "/data/hien/CAFA-6/U900\n",
      "Propagate:  True\n",
      "2it [09:49, 294.53s/it]\n",
      "CPU times: user 3.45 s, sys: 690 ms, total: 4.14 s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parse fasta files and save as feather\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_fasta.py \\\n",
    "    --config-path {CONFIG_PATH}\n",
    "\n",
    "# convert targets to parquet and calculate priors\n",
    "!{RAPIDS_ENV} protlib/scripts/create_helpers.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --batch-size 40000 \\\n",
    "    --propagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Get external data\n",
    "\n",
    "Datasets downloaded from outside and then processed. First step is downloading and parsing the datasets. After parsing, script will separate the datasets by the evidence codes. The most important split for us is kaggle/no-kaggle split. We refer `kaggle` as experimental codes, `no-kaggle` as electornic labeling, that will be used as features for the stacker models. Downloading takes quite a long time, while processing takes about 1 hour. The required structure after execution\n",
    "\n",
    "    ./temporal - extra data downloaded from http://ftp.ebi.ac.uk/pub/databases/GO/goa/old/UNIPROT/\n",
    "    ./labels   - extracted and propagated labeling\n",
    "        ./prop_test_leak_no_dup.tsv - labels from newest version\n",
    "        ./prop_test_no_kaggle.tsv   - electronic labels test\n",
    "        ./prop_train_no_kaggle.tsv  - electronic labels train\n",
    "        \n",
    "    ./cafa-terms-diff.tsv - different labels from between 2 recent versions.\n",
    "    ./prop_quickgo51.tsv  - labels from old version\n",
    "    \n",
    "    \n",
    "Other files are temporary and not needed for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, **you should run this on terminal** with **tmux**, with default 16 threads.\n",
    "\n",
    "Remember to use `conda install aria2c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download external data from ebi.ac.uk\n",
    "!{RAPIDS_ENV} protlib/scripts/downloads/dw_goant.py --config-path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, process the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270it [05:14,  1.18s/it]^C\n",
      "270it [05:14,  1.17s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/scripts/parse_go_single.py\", line 52, in <module>\n",
      "    for n, batch in tqdm.tqdm(enumerate(reader)):\n",
      "  File \"/data/hien/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1843, in __next__\n",
      "    return self.get_chunk()\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1985, in get_chunk\n",
      "    return self.read(nrows=size)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "  File \"parsers.pyx\", line 850, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
      "  File \"parsers.pyx\", line 2061, in pandas._libs.parsers.raise_parser_error\n",
      "pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n",
      "2it [00:02,  1.26s/it]^C\n",
      "2it [00:03,  1.73s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/protlib/scripts/parse_go_single.py\", line 52, in <module>\n",
      "    for n, batch in tqdm.tqdm(enumerate(reader)):\n",
      "  File \"/data/hien/.local/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1843, in __next__\n",
      "    return self.get_chunk()\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1985, in get_chunk\n",
      "    return self.read(nrows=size)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 234, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "  File \"parsers.pyx\", line 850, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\n",
      "  File \"parsers.pyx\", line 2061, in pandas._libs.parsers.raise_parser_error\n",
      "pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n"
     ]
    }
   ],
   "source": [
    "# # parse the files\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.226.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output ver226\n",
    "\n",
    "!{RAPIDS_ENV} protlib/scripts/parse_go_single.py \\\n",
    "    --file goa_uniprot_all.gaf.228.gz \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --output ver228"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is propagation. Since ebi.ac datasets contains the labeling without propagation, we will apply the rules provided in organizer's repo to labeling more terms. We will do it only for `goa_uniprot_all.gaf.228.gz` datasets since it is the actual dataset at the active competition phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "folder = BASE_PATH + '/temporal/ver228'\n",
    "\n",
    "for file in glob.glob(folder + '/labels/train*') + glob.glob(folder + '/labels/test*'):\n",
    "    name = folder + '/labels/prop_' + file.split('/')[-1]\n",
    "\n",
    "    !{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "        --path {file} \\\n",
    "        --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "        --output {name} \\\n",
    "        --device 0 \\\n",
    "        --batch_size 30000 \\\n",
    "        --batch_inner 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `cafa-terms-diff` dataset, that represents the difference between our labeling obtained by parsing `goa_uniprot_all.gaf.228.gz` dataset and `goa_uniprot_all.gaf.226.gz`. So the difference is actually the temporal. We removed duplicated protein/terms pairs from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/hien/CAFA-6/U900\n",
      "^C\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/hien/CAFA-6/U900/.//protlib/scripts/prop_tsv.py\", line 40, in <module>\n",
      "    import cupy as cp\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupy/__init__.py\", line 30, in <module>\n",
      "    import cupyx as _cupyx  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/__init__.py\", line 8, in <module>\n",
      "    from cupyx import linalg  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/linalg/__init__.py\", line 2, in <module>\n",
      "    from cupyx.linalg import sparse  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/linalg/sparse/__init__.py\", line 3, in <module>\n",
      "    from cupyx.linalg.sparse._solve import lschol  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/linalg/sparse/_solve.py\", line 6, in <module>\n",
      "    from cupyx.scipy import sparse\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/scipy/__init__.py\", line 4, in <module>\n",
      "    from cupyx.scipy.sparse._base import spmatrix as _spmatrix\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/scipy/sparse/__init__.py\", line 1, in <module>\n",
      "    from cupyx.scipy.sparse._base import issparse  # NOQA\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/cupyx/scipy/sparse/_base.py\", line 10, in <module>\n",
      "    import scipy.sparse as _sparse\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/sparse/__init__.py\", line 300, in <module>\n",
      "    from ._base import *\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/sparse/_base.py\", line 5, in <module>\n",
      "    from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/sparse/_sputils.py\", line 10, in <module>\n",
      "    from scipy._lib._util import np_long, np_ulong\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/_lib/_util.py\", line 13, in <module>\n",
      "    from scipy._lib._array_api import array_namespace, is_numpy, xp_size\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/_lib/_array_api.py\", line 18, in <module>\n",
      "    from scipy._lib.array_api_compat import (\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py\", line 1, in <module>\n",
      "    from numpy import * # noqa: F403\n",
      "  File \"<frozen importlib._bootstrap>\", line 1073, in _handle_fromlist\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/__init__.py\", line 361, in __getattr__\n",
      "    import numpy.f2py as f2py\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/f2py/__init__.py\", line 18, in <module>\n",
      "    from . import f2py2e\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/f2py/f2py2e.py\", line 19, in <module>\n",
      "    from . import crackfortran\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/site-packages/numpy/f2py/crackfortran.py\", line 3185, in <module>\n",
      "    determineexprtype_re_2 = re.compile(r'\\A[+-]?\\d+(_(?P<name>\\w+)|)\\Z', re.I)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/re.py\", line 251, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/re.py\", line 303, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 792, in compile\n",
      "    code = _code(p, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 631, in _code\n",
      "    _compile(code, p.data, flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 164, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 146, in _compile\n",
      "    _compile_charset(charset, flags, code)\n",
      "  File \"/data/hien/CAFA-6/U900/rapids-env/lib/python3.10/sre_compile.py\", line 270, in _compile_charset\n",
      "    if op is NEGATE:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/reproduce_mt.py \\\n",
    "    --path {BASE_PATH}/temporal/ver228 \\\n",
    "    --old-path {BASE_PATH}/temporal/ver226 \\\n",
    "    --new-path {BASE_PATH}/temporal/ver228 \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --target {BASE_PATH}/embeds/esm_small/train_ids.npy\n",
    "\n",
    "# # make propagation for quickgo51.tsv\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/prop_tsv.py \\\n",
    "    --path {BASE_PATH}/temporal/ver228/quickgo51.tsv \\\n",
    "    --graph {BASE_PATH}/Train/go-basic.obo \\\n",
    "    --output {BASE_PATH}/temporal/ver228/prop_quickgo51.tsv \\\n",
    "    --device 0 \\\n",
    "    --batch_size 30000 \\\n",
    "    --batch_inner 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading main submission: sub/submission-308.tsv\n",
      "Loading GOA leak...\n",
      "Loading QuickGO51...\n",
      "Loading Diff terms...\n",
      "Concatenating all sources...\n",
      "Parsing OBO graph for namespaces...\n",
      "Saving final submission to: ./sub/submission.tsv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# here we mix solutions with cafa-terms-diff and quickgo51 datasets from 1.4\n",
    "!{RAPIDS_ENV} {BASE_PATH}/protlib/scripts/postproc/make_submission2.py \\\n",
    "    --config-path {CONFIG_PATH} \\\n",
    "    --input-file \"sub/submission.tsv\" \\\n",
    "    --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "Result is stored in `./sub/submission.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P62500\tGO:0030217\t0.0265\n",
      "Q06449\tGO:0005215\t0.015\n",
      "Q9M4C1\tGO:0043229\t0.961\n",
      "P59368\tGO:0017080\t0.549\n",
      "Q9Y7B3\tGO:0045892\t0.016\n",
      "Q9FMQ1\tGO:0009416\t0.0185\n",
      "P42335\tGO:0040014\t0.004\n",
      "Q9USW8\tGO:0051130\t0.0115\n",
      "P04759\tGO:0038023\t0.91425\n",
      "Q2SX36\tGO:0009058\t0.4485\n"
     ]
    }
   ],
   "source": [
    "!head {BASE_PATH}/sub/submission.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cafa)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
