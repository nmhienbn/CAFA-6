import os
import subprocess
import argparse
import sys
import time

# --- Cáº¤U HÃŒNH ---
BASE_PATH = "."
CONFIG_PATH = "config.yaml"
LOG_DIR = "logs_ddp"
SCRIPT_PATH = f"protnn/scripts/train_gcn_ddp.py"
ENV_PYTHON = "pytorch-env/bin/python" # Python env chá»©a pytorch

# Danh sÃ¡ch ontology cáº§n cháº¡y.
# VÃ¬ cháº¡y DDP chiáº¿m toÃ n bá»™ GPU, ta sáº½ cháº¡y TUáº¦N Tá»° tá»«ng ontology.
ONTOLOGIES = ["bp"] #, "mf", "cc"]

def run_ddp_task(ontology, num_gpus, num_workers, batch_size, log_to_file):
    print(f"\n========================================================")
    print(f"ðŸš€ Báº®T Äáº¦U TRAINING DDP: {ontology.upper()}")
    print(f"   GPUs: {num_gpus} | CPU Workers/GPU: {num_workers} | Batch/GPU: {batch_size}")
    print(f"========================================================\n")
    print(f"{BASE_PATH}/{ENV_PYTHON}")
    # XÃ¢y dá»±ng lá»‡nh torchrun
    # torchrun tá»± Ä‘á»™ng quáº£n lÃ½ biáº¿n mÃ´i trÆ°á»ng cho DDP
    cmd = [
        f"{BASE_PATH}/{ENV_PYTHON}", "-m", "torch.distributed.run",
        "--nproc_per_node", str(num_gpus),
        "--master_port", "29500", # Port máº·c Ä‘á»‹nh
        f"{BASE_PATH}/{SCRIPT_PATH}",
        "--config-path", CONFIG_PATH,
        "--ontology", ontology,
        "--batch-size", str(batch_size),
        "--num-workers", str(num_workers)
    ]

    if log_to_file:
        cmd.append("--log-to-file")

    # Setup Environment
    current_env = os.environ.copy()
    abs_base_path = os.path.abspath(BASE_PATH)
    current_env["PYTHONPATH"] = f"{abs_base_path}:{current_env.get('PYTHONPATH', '')}"
    # OMP_NUM_THREADS nÃªn set tháº¥p Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t vá»›i PyTorch DataLoader workers
    current_env["OMP_NUM_THREADS"] = "1" 

    # Xá»­ lÃ½ logging
    if log_to_file:
        os.makedirs(LOG_DIR, exist_ok=True)
        log_file = os.path.join(LOG_DIR, f"ddp_train_{ontology}.log")
        print(f"ðŸ“„ Logs Ä‘ang Ä‘Æ°á»£c ghi vÃ o: {log_file}")
        
        with open(log_file, "w") as f:
            # Cháº¡y subprocess vÃ  redirect toÃ n bá»™ stdout/stderr vÃ o file
            try:
                subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, env=current_env, check=True)
                print(f"âœ… HoÃ n thÃ nh {ontology.upper()} thÃ nh cÃ´ng.")
            except subprocess.CalledProcessError:
                f.write("\n\n[FATAL ERROR] Training process crashed.\n")
                print(f"âŒ Lá»—i khi cháº¡y {ontology.upper()}. Kiá»ƒm tra file log.")
                return False
    else:
        # In trá»±c tiáº¿p ra mÃ n hÃ¬nh
        try:
            subprocess.run(cmd, env=current_env, check=True)
            print(f"âœ… HoÃ n thÃ nh {ontology.upper()} thÃ nh cÃ´ng.")
        except subprocess.CalledProcessError:
            print(f"âŒ Lá»—i khi cháº¡y {ontology.upper()}.")
            return False
            
    return True

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--gpus", type=int, default=8, help="Sá»‘ lÆ°á»£ng GPU sá»­ dá»¥ng (máº·c Ä‘á»‹nh 8)")
    parser.add_argument("--batch-size", type=int, default=4096, help="Batch size trÃªn má»—i GPU (A100 80GB -> 4096 ok)")
    parser.add_argument("--no-log", action="store_true", help="Náº¿u set flag nÃ y, sáº½ in log ra mÃ n hÃ¬nh thay vÃ¬ ghi file")
    args = parser.parse_args()

    # TÃ­nh toÃ¡n sá»‘ lÆ°á»£ng CPU worker tá»‘i Æ°u
    # Báº¡n cÃ³ 256 core, 8 GPU => 32 core/GPU.
    # Tuy nhiÃªn PyTorch dataloader cÃ³ overhead, set khoáº£ng 24 lÃ  an toÃ n vÃ  hiá»‡u quáº£.
    total_cores = os.cpu_count()
    workers_per_gpu = min(32, total_cores // args.gpus) 
    
    print(f"Há»‡ thá»‘ng cÃ³ {total_cores} cores. Sá»­ dá»¥ng {workers_per_gpu} workers cho má»—i trong sá»‘ {args.gpus} GPU.")

    start_time = time.time()

    # Cháº¡y tuáº§n tá»± tá»«ng Ontology (BP -> MF -> CC)
    # VÃ¬ má»—i cÃ¡i dÃ¹ng Full 8 GPU nÃªn pháº£i cháº¡y tuáº§n tá»±
    for ont in ONTOLOGIES:
        success = run_ddp_task(
            ontology=ont,
            num_gpus=args.gpus,
            num_workers=workers_per_gpu,
            batch_size=args.batch_size,
            log_to_file=not args.no_log
        )
        
        if not success:
            print("Dá»«ng pipeline do cÃ³ lá»—i xáº£y ra.")
            break
        
        # Nghá»‰ 5s Ä‘á»ƒ giáº£i phÃ³ng VRAM hoÃ n toÃ n trÆ°á»›c khi qua cÃ¡i má»›i
        time.sleep(5)

    print(f"\nðŸŽ‰ Tá»”NG THá»œI GIAN: {(time.time() - start_time)/60:.2f} phÃºt.")