{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNH4H5qiUJur1SPSxlY+j/v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxBIZ0_bmH2Q","executionInfo":{"status":"ok","timestamp":1765359057984,"user_tz":-420,"elapsed":9301,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"2782a737-aa41-4d0c-db90-535845134a68"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Logits ===\n","tensor([[ 0.0000,  2.0000, -1.0000],\n","        [ 3.0000, -2.0000,  0.5000]])\n","\n","=== Targets ===\n","tensor([[0., 1., 0.],\n","        [1., 0., 1.]])\n","\n","=== Gradient WITHOUT pos_weight ===\n","tensor([[ 0.0833, -0.0199,  0.0448],\n","        [-0.0079,  0.0199, -0.0629]])\n","\n","=== Gradient WITH pos_weight ===\n","tensor([[ 0.0833, -0.0993,  0.0448],\n","        [-0.0395,  0.0199, -0.1888]])\n","\n","=== Ratio (grad_pw / grad_no_pw) ===\n","tensor([[1.0000, 5.0000, 1.0000],\n","        [5.0000, 1.0000, 3.0000]])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","torch.manual_seed(0)\n","\n","# Fake logits và targets (batch=2, labels=3)\n","logits = torch.tensor([[0.0, 2.0, -1.0],\n","                       [3.0, -2.0, 0.5]], requires_grad=True)\n","\n","targets = torch.tensor([[0., 1., 0.],\n","                        [1., 0., 1.]])\n","\n","# -------------------------\n","# 1. Không có pos_weight\n","# -------------------------\n","criterion_no_pw = nn.BCEWithLogitsLoss(reduction='mean')\n","\n","loss_no_pw = criterion_no_pw(logits, targets)\n","loss_no_pw.backward()\n","\n","grad_no_pw = logits.grad.clone()  # lưu lại gradient\n","logits.grad.zero_()\n","\n","# -------------------------\n","# 2. Có pos_weight\n","# -------------------------\n","pos_weight = torch.tensor([5., 5., 3.])  # mỗi label weight=5\n","criterion_pw = nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)\n","\n","loss_pw = criterion_pw(logits, targets)\n","loss_pw.backward()\n","\n","grad_pw = logits.grad.clone()\n","\n","# -------------------------\n","# In kết quả\n","# -------------------------\n","print(\"=== Logits ===\")\n","print(logits.detach())\n","print()\n","\n","print(\"=== Targets ===\")\n","print(targets)\n","print()\n","\n","print(\"=== Gradient WITHOUT pos_weight ===\")\n","print(grad_no_pw)\n","print()\n","\n","print(\"=== Gradient WITH pos_weight ===\")\n","print(grad_pw)\n","print()\n","\n","print(\"=== Ratio (grad_pw / grad_no_pw) ===\")\n","print(grad_pw / (grad_no_pw + 1e-9))  # tránh chia 0\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# --- 1. Class ASL đã thêm pos_weight (Code từ câu trước) ---\n","class AsymmetricLossOptimized(nn.Module):\n","    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8,\n","                 disable_torch_grad_focal_loss=False, pos_weight=None):\n","        super(AsymmetricLossOptimized, self).__init__()\n","\n","        self.gamma_neg = gamma_neg\n","        self.gamma_pos = gamma_pos\n","        self.clip = clip\n","        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n","        self.eps = eps\n","\n","        self.pos_weight = pos_weight\n","        if self.pos_weight is not None:\n","             if not isinstance(self.pos_weight, torch.Tensor):\n","                 self.pos_weight = torch.tensor(self.pos_weight)\n","             self.register_buffer('weight_buffer', self.pos_weight)\n","        else:\n","            self.weight_buffer = None\n","\n","        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n","\n","    def forward(self, x, y):\n","        self.targets = y\n","        self.anti_targets = 1 - y\n","        self.xs_pos = torch.sigmoid(x)\n","        self.xs_neg = 1.0 - self.xs_pos\n","\n","        if self.clip is not None and self.clip > 0:\n","            self.xs_neg.add_(self.clip).clamp_(max=1)\n","\n","        # Tính Loss cơ bản\n","        pos_log = torch.log(self.xs_pos.clamp(min=self.eps))\n","        if self.weight_buffer is not None:\n","            pos_log = pos_log.mul(self.weight_buffer) # Nhân trọng số\n","\n","        self.loss = self.targets * pos_log\n","        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n","\n","        if self.gamma_neg > 0 or self.gamma_pos > 0:\n","            if self.disable_torch_grad_focal_loss:\n","                torch.set_grad_enabled(False)\n","            self.xs_pos = self.xs_pos * self.targets\n","            self.xs_neg = self.xs_neg * self.anti_targets\n","            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n","                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n","            if self.disable_torch_grad_focal_loss:\n","                torch.set_grad_enabled(True)\n","            self.loss *= self.asymmetric_w\n","\n","        return -self.loss.sum()\n","\n","# --- 2. Hàm chạy Test so sánh ---\n","def run_test():\n","    print(\"=== TEST START: Comparing ASL with and without pos_weight ===\\n\")\n","\n","    # GIẢ LẬP DỮ LIỆU\n","    # Batch size = 1, Num classes = 3\n","    # Logits = -2.0 -> Sigmoid(-2.0) ≈ 0.119 (Dự đoán thấp - Máy đang đoán sai)\n","    logits = torch.tensor([[-2.0, -2.0, -2.0]], requires_grad=True)\n","\n","    # Target: Class 0 là Positive (Cần tìm), Class 1, 2 là Negative\n","    targets = torch.tensor([[1.0, 0.0, 0.0]])\n","\n","    # ---------------------------------------------------------\n","    # TRƯỜNG HỢP 1: KHÔNG DÙNG POS_WEIGHT\n","    # ---------------------------------------------------------\n","    criterion_no_weight = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=0, clip=0.05, pos_weight=None)\n","\n","    # Forward\n","    loss_1 = criterion_no_weight(logits, targets)\n","\n","    # Backward (Để xem Gradient)\n","    loss_1.backward()\n","    grad_1 = logits.grad.clone() # Lưu gradient lại\n","    logits.grad.zero_() # Reset gradient cho lần test sau\n","\n","    print(f\"[Case 1 - No Weight]\")\n","    print(f\"Loss Value: {loss_1.item():.4f}\")\n","    print(f\"Gradient tại Class 0 (Positive): {grad_1[0][0]:.4f}\")\n","    print(\"-\" * 40)\n","\n","    # ---------------------------------------------------------\n","    # TRƯỜNG HỢP 2: CÓ POS_WEIGHT = 10 CHO CLASS 0\n","    # ---------------------------------------------------------\n","    # Ý nghĩa: Tôi muốn mô hình coi trọng Class 0 gấp 10 lần các class khác\n","    weights = torch.tensor([10.0, 1.0, 1.0])\n","    criterion_with_weight = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=0, clip=0.05, pos_weight=weights)\n","\n","    # Forward\n","    loss_2 = criterion_with_weight(logits, targets)\n","\n","    # Backward\n","    loss_2.backward()\n","    grad_2 = logits.grad.clone()\n","\n","    print(f\"[Case 2 - With pos_weight=[10, 1, 1]]\")\n","    print(f\"Loss Value: {loss_2.item():.4f}\")\n","    print(f\"Gradient tại Class 0 (Positive): {grad_2[0][0]:.4f}\")\n","\n","    # ---------------------------------------------------------\n","    # SO SÁNH\n","    # ---------------------------------------------------------\n","    print(\"\\n=== KẾT LUẬN ===\")\n","    ratio = grad_2[0][0] / grad_1[0][0]\n","    print(f\"Gradient tăng lên gấp: {ratio:.1f} lần\")\n","    print(\"Điều này có nghĩa là mô hình sẽ học class 0 NHANH GẤP 10 LẦN so với bình thường.\")\n","\n","if __name__ == \"__main__\":\n","    run_test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSkePA-fmQqR","executionInfo":{"status":"ok","timestamp":1765359058023,"user_tz":-420,"elapsed":31,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"d1dcc70c-bb8c-4133-ea19-86b52ed8f477"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["=== TEST START: Comparing ASL with and without pos_weight ===\n","\n","[Case 1 - No Weight]\n","Loss Value: 2.1269\n","Gradient tại Class 0 (Positive): -0.8808\n","----------------------------------------\n","[Case 2 - With pos_weight=[10, 1, 1]]\n","Loss Value: 21.2693\n","Gradient tại Class 0 (Positive): -8.8080\n","\n","=== KẾT LUẬN ===\n","Gradient tăng lên gấp: 10.0 lần\n","Điều này có nghĩa là mô hình sẽ học class 0 NHANH GẤP 10 LẦN so với bình thường.\n"]}]},{"cell_type":"code","source":["import unittest\n","import torch\n","import torch.nn as nn\n","import math\n","\n","# --- Import class loss của bạn vào đây ---\n","# Giả sử class AsymmetricLossOptimized đã được định nghĩa ở trên\n","# (Copy class AsymmetricLossOptimized vào đây hoặc import từ file khác)\n","class AsymmetricLossOptimized(nn.Module):\n","    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8,\n","                 disable_torch_grad_focal_loss=False, pos_weight=None):\n","        super(AsymmetricLossOptimized, self).__init__()\n","\n","        self.gamma_neg = gamma_neg\n","        self.gamma_pos = gamma_pos\n","        self.clip = clip\n","        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n","        self.eps = eps\n","\n","        self.pos_weight = pos_weight\n","        if self.pos_weight is not None:\n","             if not isinstance(self.pos_weight, torch.Tensor):\n","                 self.pos_weight = torch.tensor(self.pos_weight)\n","             self.register_buffer('weight_buffer', self.pos_weight)\n","        else:\n","            self.weight_buffer = None\n","\n","        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n","\n","    def forward(self, x, y):\n","        self.targets = y\n","        self.anti_targets = 1 - y\n","        self.xs_pos = torch.sigmoid(x)\n","        self.xs_neg = 1.0 - self.xs_pos\n","\n","        if self.clip is not None and self.clip > 0:\n","            self.xs_neg.add_(self.clip).clamp_(max=1)\n","\n","        pos_log = torch.log(self.xs_pos.clamp(min=self.eps))\n","        if self.weight_buffer is not None:\n","            pos_log = pos_log.mul(self.weight_buffer)\n","\n","        self.loss = self.targets * pos_log\n","        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n","\n","        if self.gamma_neg > 0 or self.gamma_pos > 0:\n","            if self.disable_torch_grad_focal_loss:\n","                torch.set_grad_enabled(False)\n","            self.xs_pos = self.xs_pos * self.targets\n","            self.xs_neg = self.xs_neg * self.anti_targets\n","            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n","                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n","            if self.disable_torch_grad_focal_loss:\n","                torch.set_grad_enabled(True)\n","            self.loss *= self.asymmetric_w\n","\n","        return -self.loss.sum()\n","\n","\n","class TestAsymmetricLoss(unittest.TestCase):\n","\n","    def setUp(self):\n","        # Setup chung: Logits và Targets giả lập\n","        self.logits = torch.tensor([[0.5, -2.0, 10.0], [-1.0, 5.0, -5.0]], requires_grad=True)\n","        self.targets = torch.tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n","        # Batch size = 2, Num classes = 3\n","\n","    def test_compare_with_bce_no_gamma_no_clip(self):\n","        \"\"\"\n","        Kịch bản 1: Khi tắt hết gamma và clip, ASL phải giống hệt BCEWithLogitsLoss (reduction='sum').\n","        \"\"\"\n","        print(\"\\n--- Test 1: Compare with standard BCE ---\")\n","\n","        # ASL settings: gamma=0, clip=0 -> Trở về BCE thuần\n","        asl_loss = AsymmetricLossOptimized(gamma_neg=0, gamma_pos=0, clip=0)\n","        bce_loss = nn.BCEWithLogitsLoss(reduction='sum')\n","\n","        loss_asl = asl_loss(self.logits, self.targets)\n","        loss_bce = bce_loss(self.logits, self.targets)\n","\n","        print(f\"ASL Loss: {loss_asl.item():.5f}\")\n","        print(f\"BCE Loss: {loss_bce.item():.5f}\")\n","\n","        # Kiểm tra độ sai lệch (dùng assert_close cho float)\n","        torch.testing.assert_close(loss_asl, loss_bce, rtol=1e-5, atol=1e-5)\n","\n","    def test_pos_weight_logic(self):\n","        \"\"\"\n","        Kịch bản 2: Kiểm tra pos_weight chỉ ảnh hưởng đến mẫu Dương, không ảnh hưởng mẫu Âm.\n","        \"\"\"\n","        print(\"\\n--- Test 2: Check pos_weight logic ---\")\n","\n","        # Chỉ xét 1 sample đơn giản: Class 0 (Pos), Class 1 (Neg)\n","        logits = torch.tensor([[0.0, 0.0]]) # Sigmoid(0) = 0.5\n","        targets = torch.tensor([[1.0, 0.0]])\n","\n","        # Case A: Không weight\n","        criterion_base = AsymmetricLossOptimized(gamma_neg=0, gamma_pos=0, clip=0, pos_weight=None)\n","        loss_base = criterion_base(logits, targets)\n","\n","        # Case B: Weight = 2 cho Class 0 (Pos)\n","        weights = torch.tensor([2.0, 10.0]) # 10.0 cho class 1 nhưng class 1 là target 0, nên k đc ảnh hưởng\n","        criterion_weighted = AsymmetricLossOptimized(gamma_neg=0, gamma_pos=0, clip=0, pos_weight=weights)\n","        loss_weighted = criterion_weighted(logits, targets)\n","\n","        # Tính toán thủ công:\n","        # Loss Pos (Target=1, p=0.5) = -log(0.5) ≈ 0.693\n","        # Loss Neg (Target=0, p=0.5) = -log(1-0.5) ≈ 0.693\n","        # Base Total = 0.693 + 0.693 = 1.386\n","\n","        # Weighted Total:\n","        # Pos được nhân 2 -> 0.693 * 2 = 1.386\n","        # Neg (dù weight=10 cũng ko đc nhân) -> 0.693\n","        # Total = 1.386 + 0.693 = 2.079\n","\n","        print(f\"Base Loss (Expected ~1.386): {loss_base.item():.4f}\")\n","        print(f\"Weighted Loss (Expected ~2.079): {loss_weighted.item():.4f}\")\n","\n","        self.assertTrue(loss_weighted > loss_base)\n","        # Kiểm tra tỷ lệ tăng có đúng logic không\n","        expected_increase = -math.log(0.5) # Lượng tăng thêm do weight=2 (thêm 1 lần log)\n","        diff = loss_weighted - loss_base\n","        self.assertAlmostEqual(diff.item(), expected_increase, places=4)\n","\n","    def test_clipping_hard_threshold(self):\n","        \"\"\"\n","        Kịch bản 3: Kiểm tra Clipping.\n","        Nếu mẫu Âm có xác suất p < clip, Loss phải bằng 0 tuyệt đối.\n","        \"\"\"\n","        print(\"\\n--- Test 3: Check Clipping (Hard Threshold) ---\")\n","\n","        # Logit = -10 -> Sigmoid(-10) ≈ 0.000045 (Rất nhỏ)\n","        logits = torch.tensor([[-10.0]], requires_grad=True)\n","        targets = torch.tensor([[0.0]]) # Negative sample\n","\n","        # Clip = 0.05 (Lớn hơn xác suất dự đoán)\n","        criterion = AsymmetricLossOptimized(gamma_neg=0, gamma_pos=0, clip=0.05)\n","        loss = criterion(logits, targets)\n","        loss.backward()\n","\n","        print(f\"Logit: -10.0, Clip: 0.05\")\n","        print(f\"Loss Value (Expect 0.0): {loss.item()}\")\n","        print(f\"Gradient (Expect 0.0): {logits.grad.item()}\")\n","\n","        self.assertEqual(loss.item(), 0.0)\n","        self.assertEqual(logits.grad.item(), 0.0)\n","\n","    def test_gamma_neg_attenuation(self):\n","        \"\"\"\n","        Kịch bản 4: Kiểm tra Gamma Negative.\n","        Gamma càng cao thì Loss của mẫu dễ (easy negative) phải càng thấp.\n","        \"\"\"\n","        print(\"\\n--- Test 4: Check Gamma Negative Attenuation ---\")\n","\n","        logits = torch.tensor([[-2.0]]) # Sigmoid(-2) ≈ 0.12 (Easy Negative)\n","        targets = torch.tensor([[0.0]])\n","\n","        # Gamma = 0\n","        crit_0 = AsymmetricLossOptimized(gamma_neg=0, clip=0)\n","        loss_0 = crit_0(logits, targets)\n","\n","        # Gamma = 4\n","        crit_4 = AsymmetricLossOptimized(gamma_neg=4, clip=0)\n","        loss_4 = crit_4(logits, targets)\n","\n","        print(f\"Loss with Gamma=0: {loss_0.item():.5f}\")\n","        print(f\"Loss with Gamma=4: {loss_4.item():.5f}\")\n","\n","        # Loss 4 phải nhỏ hơn rất nhiều so với Loss 0\n","        self.assertTrue(loss_4 < loss_0)\n","\n","        # Tính thủ công: Factor = (0.119)^4 ≈ 0.0002\n","        ratio = loss_4 / loss_0\n","        print(f\"Attenuation Ratio (Loss4/Loss0): {ratio.item():.5f}\")\n","        self.assertTrue(ratio < 0.01) # Giảm đi ít nhất 100 lần\n","\n","    def test_numerical_stability(self):\n","        \"\"\"\n","        Kịch bản 5: Kiểm tra tính ổn định số học (Numerical Stability).\n","        Với logits cực lớn hoặc cực nhỏ, Loss không được ra NaN.\n","        \"\"\"\n","        print(\"\\n--- Test 5: Check Numerical Stability ---\")\n","\n","        logits = torch.tensor([[100.0, -100.0]]) # Sigmoid(100) -> 1.0, Sigmoid(-100) -> 0.0\n","        targets = torch.tensor([[1.0, 0.0]])\n","\n","        criterion = AsymmetricLossOptimized()\n","        loss = criterion(logits, targets)\n","\n","        print(f\"Loss with extreme logits: {loss.item()}\")\n","        self.assertFalse(math.isnan(loss.item()))\n","        self.assertFalse(math.isinf(loss.item()))\n","\n","    def test_batch_consistency(self):\n","        \"\"\"\n","        Kịch bản 6: Kiểm tra tính đúng đắn khi chạy Batch.\n","        Tổng loss của batch phải bằng tổng loss của từng sample chạy riêng lẻ.\n","        \"\"\"\n","        print(\"\\n--- Test 6: Check Batch Consistency ---\")\n","\n","        l1 = torch.tensor([[0.5, -0.5]])\n","        t1 = torch.tensor([[1.0, 0.0]])\n","\n","        l2 = torch.tensor([[1.5, -2.0]])\n","        t2 = torch.tensor([[0.0, 1.0]])\n","\n","        criterion = AsymmetricLossOptimized()\n","\n","        # Chạy riêng\n","        loss_1 = criterion(l1, t1)\n","        loss_2 = criterion(l2, t2)\n","        total_separate = loss_1 + loss_2\n","\n","        # Chạy batch gộp\n","        l_batch = torch.cat([l1, l2], dim=0)\n","        t_batch = torch.cat([t1, t2], dim=0)\n","        loss_batch = criterion(l_batch, t_batch)\n","\n","        print(f\"Sum separate: {total_separate.item():.5f}\")\n","        print(f\"Batch loss: {loss_batch.item():.5f}\")\n","\n","        torch.testing.assert_close(loss_batch, total_separate, rtol=1e-5, atol=1e-5)\n","\n","    def test_batch_pos_weight_logic(self):\n","        \"\"\"\n","        Kịch bản 7: Kiểm tra pos_weight hoạt động đúng trên BATCH lớn.\n","        Đảm bảo broadcasting hoạt động chính xác (Trọng số class nào ăn vào class đó).\n","        \"\"\"\n","        print(\"\\n--- Test 7: Check Batch processing with pos_weight ---\")\n","\n","        # CẤU HÌNH DỮ LIỆU\n","        # Batch size = 2, Num Classes = 2\n","        # Logits = 0.0 -> Sigmoid(0.0) = 0.5\n","        # Việc dùng 0.5 giúp dễ tính nhẩm: -log(0.5) ≈ 0.6931\n","        logits = torch.zeros((2, 2))\n","\n","        # Targets:\n","        # Sample 1: Class 0 là Dương, Class 1 là Âm\n","        # Sample 2: Class 0 là Âm, Class 1 là Dương\n","        targets = torch.tensor([\n","            [1.0, 0.0],\n","            [0.0, 1.0]\n","        ])\n","\n","        # Weights:\n","        # Class 0: Trọng số 2.0\n","        # Class 1: Trọng số 3.0\n","        pos_weight = torch.tensor([2.0, 3.0])\n","\n","        # KHỞI TẠO LOSS\n","        # Tắt gamma và clip để chỉ test logic của pos_weight + BCE\n","        criterion = AsymmetricLossOptimized(gamma_neg=0, gamma_pos=0, clip=0, pos_weight=pos_weight)\n","\n","        # TÍNH TOÁN\n","        actual_loss = criterion(logits, targets)\n","\n","        # TÍNH TAY (EXPECTED VALUE)\n","        base_loss = -math.log(0.5) # ≈ 0.693147\n","\n","        # Sample 1:\n","        # - Class 0 (Pos, w=2): 2.0 * base_loss\n","        # - Class 1 (Neg, ko w): 1.0 * base_loss\n","        loss_s1 = (2.0 * base_loss) + base_loss\n","\n","        # Sample 2:\n","        # - Class 0 (Neg, ko w): 1.0 * base_loss\n","        # - Class 1 (Pos, w=3): 3.0 * base_loss\n","        loss_s2 = base_loss + (3.0 * base_loss)\n","\n","        expected_total_loss = loss_s1 + loss_s2\n","\n","        print(f\"Sample 1 Loss (Expected ~ {3 * 0.693:.2f}): {loss_s1:.4f}\")\n","        print(f\"Sample 2 Loss (Expected ~ {4 * 0.693:.2f}): {loss_s2:.4f}\")\n","        print(f\"Total Batch Loss: {actual_loss.item():.4f}\")\n","\n","        # SO SÁNH\n","        # Kiểm tra xem code có ra đúng con số (2+1 + 1+3) = 7 lần base_loss không\n","        self.assertAlmostEqual(actual_loss.item(), expected_total_loss, places=4)\n","\n","if __name__ == '__main__':\n","    # argv=['first-arg-is-ignored']: Bỏ qua các tham số hệ thống của Colab\n","    # exit=False: Ngăn không cho Colab bị tắt (crash) sau khi chạy xong test\n","    unittest.main(argv=['first-arg-is-ignored'], exit=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1qUKORzmSKI","executionInfo":{"status":"ok","timestamp":1765359058356,"user_tz":-420,"elapsed":327,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"34fbbb06-f55d-416f-b0df-91910178c095"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["....."]},{"output_type":"stream","name":"stdout","text":["\n","--- Test 6: Check Batch Consistency ---\n","Sum separate: 2.56346\n","Batch loss: 2.56346\n","\n","--- Test 7: Check Batch processing with pos_weight ---\n","Sample 1 Loss (Expected ~ 2.08): 2.0794\n","Sample 2 Loss (Expected ~ 2.77): 2.7726\n","Total Batch Loss: 4.8520\n","\n","--- Test 3: Check Clipping (Hard Threshold) ---\n","Logit: -10.0, Clip: 0.05\n","Loss Value (Expect 0.0): -0.0\n","Gradient (Expect 0.0): -0.0\n","\n","--- Test 1: Compare with standard BCE ---\n","ASL Loss: 0.92774\n","BCE Loss: 0.92774\n","\n","--- Test 4: Check Gamma Negative Attenuation ---\n","Loss with Gamma=0: 0.12693\n","Loss with Gamma=4: 0.00003\n","Attenuation Ratio (Loss4/Loss0): 0.00020\n"]},{"output_type":"stream","name":"stderr","text":["..\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.026s\n","\n","OK\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Test 5: Check Numerical Stability ---\n","Loss with extreme logits: -0.0\n","\n","--- Test 2: Check pos_weight logic ---\n","Base Loss (Expected ~1.386): 1.3863\n","Weighted Loss (Expected ~2.079): 2.0794\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eNyW83IjmUIg"},"execution_count":null,"outputs":[]}]}