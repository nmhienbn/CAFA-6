{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMPWXwAtEX4m7uUYycd6gKz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3d73e0ff20f1445f8bc16261cb1f8063":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b066a6479f74004a92df45c963b3618","IPY_MODEL_5c2ac2615f32407daf034b0c71ccec69","IPY_MODEL_92df20d745f343579b15a6410bd61b95"],"layout":"IPY_MODEL_7e81a58e5d074e7095244c24f26b2449"}},"4b066a6479f74004a92df45c963b3618":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb4f3e0bde964d09be28d0f526763d62","placeholder":"‚Äã","style":"IPY_MODEL_e91cc8278adb4e0fa3d8f78592a55b71","value":"Inference:‚Äá100%"}},"5c2ac2615f32407daf034b0c71ccec69":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ec3d5e569ba421ca26782aaabe3ac01","max":1753,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b862e243aa14a5db0c3d1c52583ab81","value":1753}},"92df20d745f343579b15a6410bd61b95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c3bc63645c6445aa9c3260aed025b1b","placeholder":"‚Äã","style":"IPY_MODEL_88f49595fb444029959cae01b5427048","value":"‚Äá1753/1753‚Äá[01:07&lt;00:00,‚Äá38.59it/s]"}},"7e81a58e5d074e7095244c24f26b2449":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4f3e0bde964d09be28d0f526763d62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e91cc8278adb4e0fa3d8f78592a55b71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ec3d5e569ba421ca26782aaabe3ac01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b862e243aa14a5db0c3d1c52583ab81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c3bc63645c6445aa9c3260aed025b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88f49595fb444029959cae01b5427048":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6401a4edd126468fba867d0e0d65e433":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_378321947ee94b8c99d185c913e3a1e2","IPY_MODEL_c1f48ac0771242ec9e80d3fc49b2cba7","IPY_MODEL_829c9f22f314420eb31e888d24b3e784"],"layout":"IPY_MODEL_d657fb971c204ec2aed9f931372abf5f"}},"378321947ee94b8c99d185c913e3a1e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cca2a1a7c5f4177b301d64f4e21e8f4","placeholder":"‚Äã","style":"IPY_MODEL_4da7acbb9b824f8fbae4fc00b573f534","value":"Inferring:‚Äá100%"}},"c1f48ac0771242ec9e80d3fc49b2cba7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a8efe6359ee471bb8d82903b7acb83c","max":877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_117d474eae5f4e54b316296b12ba3f3b","value":877}},"829c9f22f314420eb31e888d24b3e784":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5596e3b6259a45cc9b22a43f00ee3b1c","placeholder":"‚Äã","style":"IPY_MODEL_fc6addaaf5e14a23a26b70d080dcb073","value":"‚Äá877/877‚Äá[00:58&lt;00:00,‚Äá13.52it/s]"}},"d657fb971c204ec2aed9f931372abf5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cca2a1a7c5f4177b301d64f4e21e8f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da7acbb9b824f8fbae4fc00b573f534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a8efe6359ee471bb8d82903b7acb83c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"117d474eae5f4e54b316296b12ba3f3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5596e3b6259a45cc9b22a43f00ee3b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc6addaaf5e14a23a26b70d080dcb073":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S2KUCRMBo3aQ","executionInfo":{"status":"ok","timestamp":1765857209874,"user_tz":-420,"elapsed":29278,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"a1a20694-a38d-41e5-bcb4-e08f9bd0278d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h‚úÖ ƒê√£ c√†i ƒë·∫∑t xong th∆∞ vi·ªán!\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q transformers biopython kaggle\n","print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t xong th∆∞ vi·ªán!\")"]},{"cell_type":"code","source":["import os\n","from google.colab import files\n","\n","# Upload file kaggle.json\n","print(\"Vui l√≤ng upload file kaggle.json c·ªßa b·∫°n:\")\n","files.upload()\n","\n","# C·∫•u h√¨nh Kaggle API\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# T·∫£i d·ªØ li·ªáu cu·ªôc thi (S·∫Ω m·∫•t kho·∫£ng 1-2 ph√∫t)\n","# print(\"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu CAFA 6...\")\n","# !kaggle competitions download -c cafa-6-protein-function-prediction\n","# !unzip -q cafa-6-protein-function-prediction.zip -d /content/cafa6_data\n","# print(\"‚úÖ ƒê√£ t·∫£i v√† gi·∫£i n√©n d·ªØ li·ªáu t·∫°i /content/cafa6_data\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"XrsY5J0Vo5aL","executionInfo":{"status":"ok","timestamp":1765859715587,"user_tz":-420,"elapsed":6948,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"bf2f289f-e059-4834-9b06-273fc862dbe1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Vui l√≤ng upload file kaggle.json c·ªßa b·∫°n:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d884ee1f-7fe8-4e22-8db2-f015ad404166\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d884ee1f-7fe8-4e22-8db2-f015ad404166\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import gc\n","import os\n","import sys\n","from tqdm.auto import tqdm\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","CONFIG = {\n","    'EMBEDDINGS': {\n","        'ankh': {\n","            'train': '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_train_FINAL_embeddings.npy',\n","            'test':  '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_test_FINAL_embeddings.npy',\n","        },\n","        'esm': {\n","            'train': '/content/drive/MyDrive/CAFA6_Embeddings/ESM2_Aligned_Ankh/esm2_train_FINAL_embeddings.npy',\n","            'test':  '/content/drive/MyDrive/CAFA6_Embeddings/ESM2_Aligned_Ankh/esm2_test_FINAL_embeddings.npy',\n","        },\n","        'prot_t5': {\n","            'train': '/content/drive/MyDrive/CAFA6_Embeddings/Prot_T5_Aligned/protT5_xl_train_FINAL_embeddings.npy',\n","            'test':  '/content/drive/MyDrive/CAFA6_Embeddings/Prot_T5_Aligned/protT5_xl_test_FINAL_embeddings.npy',\n","        },\n","        # 'protBERT': {\n","        #     'train': '/content/drive/MyDrive/CAFA6_Embeddings/protBERT_Aligned/protBERT_train_FINAL_embeddings.npy',\n","        #     'test':  '/content/drive/MyDrive/CAFA6_Embeddings/protBERT_Aligned/protBERT_test_FINAL_embeddings.npy',\n","        # },\n","    },\n","\n","    'TRAIN_ID_PATH': '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_train_FINAL_ids.npy',\n","    'TEST_ID_PATH':  '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_test_FINAL_ids.npy',\n","\n","    'SAVE_DIR': '/content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run',\n","    'TRAIN_TERMS': '/content/cafa6_data/Train/train_terms.tsv',\n","    'IA_FILE': '/content/cafa6_data/IA.tsv',\n","    'TEST_FASTA': '/content/cafa6_data/Test/testsuperset.fasta',\n","    'OBO_FILE': '/content/cafa6_data/Train/go-basic.obo',\n","\n","    # Model Params\n","    'TOP_K_LABELS': 10000, # Gi·ªØ m·ª©c n√†y ƒë·ªÉ model h·ªçc t·ªët\n","    # 'MIN_FREQ': 3,\n","\n","    # Input s·∫Ω l√† 768 (Ankh) + 1280 (ESM) = 2048\n","    'ENCODER_LAYERS': [2048, 1024, 512],\n","\n","    'DROPOUT_RATE': 0.4, # TƒÉng dropout v√¨ input l·ªõn\n","    'EPOCHS': 50,\n","    'BATCH_SIZE': 128,\n","    'LEARNING_RATE': 2e-4, # Gi·∫£m LR m·ªôt ch√∫t cho ·ªïn ƒë·ªãnh\n","    'LABEL_SMOOTHING': 0.1,\n","\n","    # Inference Params (Chu·∫©n 0.27 ƒëi·ªÉm)\n","    'MIN_CONFIDENCE': 0.01,\n","    'MAX_PREDS_PER_PROTEIN': 20,\n","\n","    'SEED': 42,\n","    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu'\n","}\n","\n","os.makedirs(CONFIG['SAVE_DIR'], exist_ok=True)\n","print(f\"üöÄ CAFA 6 - DUAL MODEL (ANKH + ESM) | Device: {CONFIG['DEVICE']}\")\n","\n","torch.manual_seed(CONFIG['SEED'])\n","np.random.seed(CONFIG['SEED'])\n","\n","def load_go_aspect_mapping(obo_path):\n","    print(f\"üìñ Parsing OBO file: {obo_path}...\")\n","    namespace_map = {\n","        'biological_process': 'BPO',\n","        'molecular_function': 'MFO',\n","        'cellular_component': 'CCO'\n","    }\n","    mapping = {}\n","    current_id = \"\"\n","    with open(obo_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('id: GO:'):\n","                current_id = line.split('id: ')[1]\n","            elif line.startswith('namespace:'):\n","                if current_id:\n","                    full_ns = line.split('namespace: ')[1]\n","                    mapping[current_id] = namespace_map.get(full_ns, 'UNKNOWN')\n","    print(f\"‚úÖ Loaded aspect mapping for {len(mapping):,} terms.\")\n","    return mapping\n","\n","# ============================================================================\n","# 1. MEMORY-SAFE DATASET (CH√åA KH√ìA ƒê·ªÇ KH√îNG TR√ÄN RAM)\n","# ============================================================================\n","class MultiSourceDataset(Dataset):\n","    def __init__(self, embedding_paths_dict, y_tensor=None, indices=None):\n","        self.mmaps = {}\n","        self.keys = list(embedding_paths_dict.keys())\n","\n","        # Load mmap\n","        for name, path in embedding_paths_dict.items():\n","            self.mmaps[name] = np.load(path, mmap_mode='r')\n","\n","        # Base length\n","        first_key = self.keys[0]\n","        self.total_len = len(self.mmaps[first_key])\n","\n","        self.indices = indices if indices is not None else np.arange(self.total_len)\n","        self.y = y_tensor\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        real_idx = self.indices[idx]\n","\n","        inputs = []\n","        # Load t·ª´ng vector -> List of Tensors\n","        for key in self.keys:\n","            vec = torch.from_numpy(self.mmaps[key][real_idx].copy()).float()\n","            inputs.append(vec)\n","\n","        if self.y is not None:\n","            return inputs, self.y[real_idx]\n","        return (inputs,)\n","\n","# ============================================================================\n","# 2. LOAD DATA & PROCESS LABELS\n","# ============================================================================\n","print(\"\\n[1/5] Checking Files...\")\n","train_ids = np.load(CONFIG['TRAIN_ID_PATH'])\n","print(f\"   ‚úì Train IDs: {len(train_ids)}\")\n","\n","print(\"\\n[2/5] Processing Labels (IA Strategy)...\")\n","# 1. Load Terms & IA\n","df_terms = pd.read_csv(CONFIG['TRAIN_TERMS'], sep='\\t', header=0, names=['EntryID', 'term', 'aspect'])\n","df_ia = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'])\n","ia_dict = dict(zip(df_ia['term'], df_ia['ia']))\n","\n","# 2. T√≠nh Score\n","term_counts = df_terms['term'].value_counts().reset_index()\n","term_counts.columns = ['term', 'freq']\n","\n","# print(f\"   Original terms count: {len(term_counts)}\")\n","# term_counts = term_counts[term_counts['freq'] >= CONFIG['MIN_FREQ']]\n","# print(f\"   -> After filtering (Freq >= {CONFIG['MIN_FREQ']}): {len(term_counts)} terms\")\n","\n","term_counts['ia'] = term_counts['term'].map(ia_dict).fillna(0.0)\n","term_counts['score'] = term_counts['freq'] * term_counts['ia']\n","\n","# 3. Ch·ªçn Top K\n","top_terms_df = term_counts.sort_values(by='score', ascending=False).head(CONFIG['TOP_K_LABELS'])\n","top_terms = top_terms_df['term'].tolist()\n","print(f\"   ‚úì Selected {len(top_terms)} terms.\")\n","\n","obo_path = CONFIG['OBO_FILE']\n","go_aspect_map = load_go_aspect_mapping(obo_path)\n","\n","aspect_groups = {\n","    'MFO': [],\n","    'BPO': [],\n","    'CCO': []\n","}\n","\n","for term in top_terms:\n","    aspect = go_aspect_map.get(term, 'UNKNOWN')\n","    if aspect in aspect_groups:\n","        aspect_groups[aspect].append(term)\n","\n","print(f\"üìä Aspect Distribution:\")\n","print(f\"   - MFO (Molecular Function): {len(aspect_groups['MFO']):,} labels\")\n","print(f\"   - CCO (Cellular Component): {len(aspect_groups['CCO']):,} labels\")\n","print(f\"   - BPO (Biological Process): {len(aspect_groups['BPO']):,} labels\")\n","\n","# 4. Map ID -> Terms\n","df_filtered = df_terms[df_terms['term'].isin(top_terms)]\n","id_to_terms = df_filtered.groupby('EntryID')['term'].apply(list).to_dict()\n","\n","# D·ªçn d·∫πp\n","del df_terms, df_filtered, df_ia, term_counts, top_terms_df\n","gc.collect()\n","\n","# ============================================================================\n","# 5. MODEL & TRAINING\n","# ============================================================================\n","print(\"\\n[5/5] Building Model...\")\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, in_dim, layers_config, dropout):\n","        super().__init__()\n","        layers = []\n","\n","        # 1. LAYER NORM ƒê·∫¶U V√ÄO (B·∫ÆT BU·ªòC)\n","        # ƒê·ªÉ c√¢n b·∫±ng '√¢m l∆∞·ª£ng' gi·ªØa ESM (h√©t to) v√† Ankh (n√≥i nh·ªè)\n","        layers.append(nn.LayerNorm(in_dim))\n","\n","        prev_dim = in_dim\n","        for i, dim in enumerate(layers_config):\n","            layers.extend([\n","                nn.Linear(prev_dim, dim),\n","                nn.BatchNorm1d(dim), # ·ªîn ƒë·ªãnh training\n","                nn.GELU(),           # Hi·ªán ƒë·∫°i h∆°n ReLU\n","                nn.Dropout(dropout)\n","            ])\n","            prev_dim = dim\n","\n","        self.net = nn.Sequential(*layers)\n","        self.out_dim = prev_dim\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Module l·ªçc nhi·ªÖu (Attention)\n","class SEBlock(nn.Module):\n","    \"\"\"Squeeze-and-Excitation Block ƒë·ªÉ l·ªçc nhi·ªÖu sau khi g·ªôp\"\"\"\n","    def __init__(self, in_dim, reduction=4):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_dim, in_dim // reduction, bias=False),\n","            nn.ReLU(),\n","            nn.Linear(in_dim // reduction, in_dim, bias=False),\n","            nn.Sigmoid() # T·∫°o ra mask t·ª´ 0 ƒë·∫øn 1\n","        )\n","\n","    def forward(self, x):\n","        # x shape: [Batch, Dim]\n","        # Attention weight: [Batch, Dim]\n","        w = self.fc(x)\n","        # Nh√¢n tr·ªçng s·ªë v√†o x: C√°i n√†o quan tr·ªçng th√¨ gi·ªØ, r√°c th√¨ nh√¢n v·ªõi 0\n","        return x * w\n","\n","class MultiModalNet(nn.Module):\n","    def __init__(self, input_dims_list, encoder_layers, dropout, num_classes):\n","        super().__init__()\n","\n","        self.encoders = nn.ModuleList()\n","        self.fusion_input_dim = 0\n","\n","        print(\"\\nüèóÔ∏è Building Advanced Architecture:\")\n","\n","        # 1. X√¢y d·ª±ng c√°c nh√°nh Encoder\n","        for i, in_dim in enumerate(input_dims_list):\n","            print(f\"   ‚û§ Branch {i+1}: Input {in_dim} -> Output {encoder_layers[-1]}\")\n","            enc = EncoderBlock(in_dim, encoder_layers, dropout)\n","            self.encoders.append(enc)\n","            self.fusion_input_dim += enc.out_dim\n","\n","        print(f\"   ‚û§ Fusion Dim: {self.fusion_input_dim}\")\n","\n","        # 2. SE-Block (B·ªô l·ªçc th√¥ng minh)\n","        self.attention_filter = SEBlock(self.fusion_input_dim)\n","\n","        # 3. Layer t·ªïng h·ª£p cu·ªëi c√πng\n","        self.head = nn.Sequential(\n","            nn.BatchNorm1d(self.fusion_input_dim),\n","            nn.Linear(self.fusion_input_dim, 512),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","\n","            # Th√™m m·ªôt l·ªõp n·ªØa ƒë·ªÉ tƒÉng kh·∫£ nƒÉng h·ªçc\n","            nn.Linear(512, 512),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, inputs_list):\n","        features = []\n","        # ƒêi qua t·ª´ng nh√°nh\n","        for i, encoder in enumerate(self.encoders):\n","            feat = encoder(inputs_list[i])\n","            features.append(feat)\n","\n","        # G·ªôp l·∫°i (Concatenate)\n","        combined = torch.cat(features, dim=1)\n","\n","        # L·ªåC NHI·ªÑU (ƒêi·ªÉm kh√°c bi·ªát l·ªõn nh·∫•t)\n","        # M·∫°ng s·∫Ω t·ª± h·ªçc c√°ch \"t·∫Øt ti·∫øng\" c√°c ƒë·∫∑c tr∆∞ng r√°c t·ª´ ProtT5 n·∫øu n√≥ th·∫•y kh√¥ng c·∫ßn thi·∫øt\n","        refined = self.attention_filter(combined)\n","\n","        return self.head(refined)\n","\n","train_paths = {k: v['train'] for k, v in CONFIG['EMBEDDINGS'].items()}\n","aspect_names = ['MFO', 'CCO', 'BPO']\n","temp_dataset = MultiSourceDataset(train_paths, indices=[0])\n","sample_inputs = temp_dataset[0][0] # Get inputs of first sample\n","INPUT_DIMS_LIST = [x.shape[0] for x in sample_inputs]\n","print(f\"   ‚úì Detected Input Dims: {INPUT_DIMS_LIST}\")\n","del temp_dataset\n","\n","# L∆∞u th√¥ng tin ƒë·ªÉ d√πng cho Inference\n","trained_models_meta = []\n","\n","for aspect in aspect_names:\n","    terms = aspect_groups[aspect]\n","    if len(terms) == 0: continue\n","\n","    print(f\"\\n\" + \"=\"*60)\n","    print(f\"üß¨ TRAINING ASPECT: {aspect} ({len(terms)} labels)\")\n","    print(\"=\"*60)\n","\n","    # --- A. PREPARE Y TENSOR CHO ASPECT N√ÄY ---\n","    print(f\"   Shape mapping for {aspect}...\")\n","    mlb = MultiLabelBinarizer(classes=terms, sparse_output=True)\n","    mlb.fit([terms])\n","\n","    # Ch·ªâ transform c√°c nh√£n thu·ªôc Aspect n√†y\n","    # L∆∞u √Ω: id_to_terms ch·ª©a t·∫•t c·∫£, mlb s·∫Ω t·ª± ignore c√°c nh√£n kh√¥ng thu·ªôc 'terms'\n","    import warnings\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\")\n","        y_labels_list = [id_to_terms.get(pid, []) for pid in train_ids]\n","        y_sparse = mlb.transform(y_labels_list)\n","\n","    # Convert to Tensor\n","    y_binary = y_sparse.astype(np.float32).toarray()\n","    y_tensor = torch.from_numpy(y_binary)\n","\n","    # Label Smoothing\n","    if CONFIG['LABEL_SMOOTHING'] > 0:\n","        y_tensor.mul_(1 - CONFIG['LABEL_SMOOTHING']).add_(CONFIG['LABEL_SMOOTHING'] / len(terms))\n","\n","    # Pos Weight (Optional - Re-calculate per aspect)\n","    weights_list = [ia_dict.get(t, 0.0) for t in terms]\n","    pos_weight = torch.tensor(weights_list, dtype=torch.float32).to(CONFIG['DEVICE'])\n","\n","    # --- B. DATALOADERS ---\n","    indices = np.arange(len(y_tensor))\n","    train_idx, val_idx = train_test_split(indices, test_size=0.15, random_state=CONFIG['SEED'])\n","\n","    train_dataset = MultiSourceDataset(train_paths, y_tensor, indices=train_idx)\n","    val_dataset   = MultiSourceDataset(train_paths, y_tensor, indices=val_idx)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\n","    val_loader   = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n","\n","    # Auto detect input dims (Ch·ªâ l√†m l·∫ßn ƒë·∫ßu ho·∫∑c l√†m l·∫°i c≈©ng ƒë∆∞·ª£c)\n","    # sample_inputs, _ = train_dataset[0]\n","    # INPUT_DIMS_LIST = [x.shape[0] for x in sample_inputs]\n","\n","    # --- C. BUILD MODEL ---\n","    # Reset model m·ªõi cho m·ªói Aspect\n","    model = MultiModalNet(\n","        input_dims_list = INPUT_DIMS_LIST,\n","        encoder_layers  = CONFIG['ENCODER_LAYERS'],\n","        dropout         = CONFIG['DROPOUT_RATE'],\n","        num_classes     = len(terms) # Output dim thay ƒë·ªïi theo Aspect\n","    ).to(CONFIG['DEVICE'])\n","\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=0.01)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","\n","    # --- D. TRAIN ---\n","    best_val_loss = float('inf')\n","    model_save_path = f\"{CONFIG['SAVE_DIR']}/model_{aspect}.pth\"\n","\n","    for epoch in range(CONFIG['EPOCHS']):\n","        model.train()\n","        train_loss = 0\n","        for X_b, y_b in train_loader:\n","            X_b, y_b = [x.to(CONFIG['DEVICE']) for x in X_b], y_b.to(CONFIG['DEVICE'])\n","            optimizer.zero_grad()\n","            logits = model(X_b)\n","            loss = criterion(logits, y_b)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        avg_train = train_loss / len(train_loader)\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for X_b, y_b in val_loader:\n","                X_b, y_b = [x.to(CONFIG['DEVICE']) for x in X_b], y_b.to(CONFIG['DEVICE'])\n","                logits = model(X_b)\n","                loss = criterion(logits, y_b)\n","                val_loss += loss.item()\n","\n","        avg_val = val_loss / len(val_loader)\n","        scheduler.step(avg_val)\n","\n","        print(f\"   Epoch {epoch+1:02d} | Train: {avg_train:.4f} | Val: {avg_val:.4f}\")\n","\n","        if avg_val < best_val_loss:\n","            best_val_loss = avg_val\n","            torch.save(model.state_dict(), model_save_path)\n","\n","    print(f\"   üèÜ Saved best {aspect} model to {model_save_path}\")\n","\n","    # L∆∞u meta data\n","    trained_models_meta.append({\n","        'aspect': aspect,\n","        'path': model_save_path,\n","        'terms': terms,\n","        'num_classes': len(terms)\n","    })\n","\n","    # D·ªçn d·∫πp RAM\n","    del model, train_loader, val_loader, train_dataset, val_dataset, y_tensor, y_sparse\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# ============================================================================\n","# 4. INFERENCE (MULTI-MODEL MERGE)\n","# ============================================================================\n","print(\"\\nüîÆ PREDICTING WITH MULTIPLE MODELS...\")\n","\n","test_paths = {k: v['test'] for k, v in CONFIG['EMBEDDINGS'].items()}\n","test_ids = np.load(CONFIG['TEST_ID_PATH'])\n","\n","# Dataset Test (D√πng chung cho c·∫£ 3 model)\n","test_dataset = MultiSourceDataset(test_paths)\n","test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n","\n","# Load 3 Model v√†o list (Deep Learning model nh·∫π, load c·∫£ 3 v√†o VRAM ok)\n","# N·∫øu VRAM qu√° y·∫øu (T4 th∆∞·ªùng ch·ªãu ƒë∆∞·ª£c), th√¨ c√≥ th·ªÉ load-predict-unload t·ª´ng c√°i\n","loaded_models = []\n","all_combined_terms = []\n","\n","print(\"   Loading models...\")\n","for meta in trained_models_meta:\n","    # Build l·∫°i ki·∫øn tr√∫c\n","    m = MultiModalNet(\n","        input_dims_list = INPUT_DIMS_LIST,\n","        encoder_layers  = CONFIG['ENCODER_LAYERS'],\n","        dropout         = CONFIG['DROPOUT_RATE'],\n","        num_classes     = meta['num_classes']\n","    ).to(CONFIG['DEVICE'])\n","\n","    m.load_state_dict(torch.load(meta['path']))\n","    m.eval()\n","    loaded_models.append(m)\n","    all_combined_terms.extend(meta['terms']) # Gh√©p t√™n nh√£n n·ªëi ti·∫øp nhau\n","\n","print(f\"   Total combined labels: {len(all_combined_terms)}\")\n","\n","submission_path = f\"{CONFIG['SAVE_DIR']}/submission.tsv\"\n","n_predictions = 0\n","current_idx = 0\n","\n","with open(submission_path, 'w') as f:\n","    with torch.no_grad():\n","        for (X_b,) in tqdm(test_loader, desc=\"Inference\"):\n","            X_b = [x.to(CONFIG['DEVICE']) for x in X_b]\n","\n","            # 1. Predict t·ª´ng Model\n","            batch_preds_list = []\n","            for model in loaded_models:\n","                logits = model(X_b)\n","                probs = torch.sigmoid(logits).cpu().numpy() # [Batch, Num_Classes_Aspect]\n","                batch_preds_list.append(probs)\n","\n","            # 2. Gh√©p ngang k·∫øt qu·∫£ (Concatenate)\n","            # [Batch, MFO] + [Batch, CCO] + [Batch, BPO] -> [Batch, Total]\n","            batch_full_probs = np.hstack(batch_preds_list)\n","\n","            # 3. Ghi file (Logic c≈©)\n","            ids_batch = test_ids[current_idx : current_idx + len(batch_full_probs)]\n","            current_idx += len(batch_full_probs)\n","\n","            for i, pid in enumerate(ids_batch):\n","                probs = batch_full_probs[i]\n","\n","                # TOP-K\n","                top_k = CONFIG['MAX_PREDS_PER_PROTEIN']\n","                if len(probs) > top_k:\n","                    ind = np.argpartition(probs, -top_k)[-top_k:]\n","                else:\n","                    ind = np.arange(len(probs))\n","\n","                ind = ind[np.argsort(probs[ind])][::-1]\n","\n","                for idx in ind:\n","                    score = probs[idx]\n","                    if score > CONFIG['MIN_CONFIDENCE']:\n","                        # Map ƒë√∫ng index t·ªïng\n","                        term = all_combined_terms[idx]\n","                        f.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n","                        n_predictions += 1\n","\n","            del batch_full_probs, batch_preds_list, X_b\n","\n","print(f\"\\n‚úÖ DONE! File: {submission_path}\")\n","print(f\"‚úÖ Total Predictions: {n_predictions:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3d73e0ff20f1445f8bc16261cb1f8063","4b066a6479f74004a92df45c963b3618","5c2ac2615f32407daf034b0c71ccec69","92df20d745f343579b15a6410bd61b95","7e81a58e5d074e7095244c24f26b2449","fb4f3e0bde964d09be28d0f526763d62","e91cc8278adb4e0fa3d8f78592a55b71","4ec3d5e569ba421ca26782aaabe3ac01","7b862e243aa14a5db0c3d1c52583ab81","9c3bc63645c6445aa9c3260aed025b1b","88f49595fb444029959cae01b5427048"]},"id":"BK6rw0P3pAEL","executionInfo":{"status":"ok","timestamp":1765815304381,"user_tz":-420,"elapsed":2512221,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"bcc32f02-e55a-4291-aae9-5697ba88f833"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ CAFA 6 - DUAL MODEL (ANKH + ESM) | Device: cuda\n","\n","[1/5] Checking Files...\n","   ‚úì Train IDs: 82404\n","\n","[2/5] Processing Labels (IA Strategy)...\n","   ‚úì Selected 10000 terms.\n","üìñ Parsing OBO file: /content/cafa6_data/Train/go-basic.obo...\n","‚úÖ Loaded aspect mapping for 48,101 terms.\n","üìä Aspect Distribution:\n","   - MFO (Molecular Function): 3,465 labels\n","   - CCO (Cellular Component): 1,612 labels\n","   - BPO (Biological Process): 4,923 labels\n","\n","[5/5] Building Model...\n","   ‚úì Detected Input Dims: [768, 1280, 1024]\n","\n","============================================================\n","üß¨ TRAINING ASPECT: MFO (3465 labels)\n","============================================================\n","   Shape mapping for MFO...\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","   Epoch 01 | Train: 0.0355 | Val: 0.0057\n","   Epoch 02 | Train: 0.0061 | Val: 0.0056\n","   Epoch 03 | Train: 0.0059 | Val: 0.0054\n","   Epoch 04 | Train: 0.0057 | Val: 0.0053\n","   Epoch 05 | Train: 0.0055 | Val: 0.0051\n","   Epoch 06 | Train: 0.0054 | Val: 0.0050\n","   Epoch 07 | Train: 0.0052 | Val: 0.0049\n","   Epoch 08 | Train: 0.0050 | Val: 0.0048\n","   Epoch 09 | Train: 0.0049 | Val: 0.0047\n","   Epoch 10 | Train: 0.0048 | Val: 0.0046\n","   Epoch 11 | Train: 0.0047 | Val: 0.0045\n","   Epoch 12 | Train: 0.0045 | Val: 0.0044\n","   Epoch 13 | Train: 0.0044 | Val: 0.0044\n","   Epoch 14 | Train: 0.0043 | Val: 0.0043\n","   Epoch 15 | Train: 0.0043 | Val: 0.0043\n","   Epoch 16 | Train: 0.0042 | Val: 0.0042\n","   Epoch 17 | Train: 0.0041 | Val: 0.0042\n","   Epoch 18 | Train: 0.0040 | Val: 0.0041\n","   Epoch 19 | Train: 0.0040 | Val: 0.0041\n","   Epoch 20 | Train: 0.0039 | Val: 0.0041\n","   Epoch 21 | Train: 0.0039 | Val: 0.0041\n","   Epoch 22 | Train: 0.0038 | Val: 0.0040\n","   Epoch 23 | Train: 0.0037 | Val: 0.0040\n","   Epoch 24 | Train: 0.0037 | Val: 0.0040\n","   Epoch 25 | Train: 0.0037 | Val: 0.0040\n","   Epoch 26 | Train: 0.0036 | Val: 0.0040\n","   Epoch 27 | Train: 0.0036 | Val: 0.0040\n","   Epoch 28 | Train: 0.0036 | Val: 0.0040\n","   Epoch 29 | Train: 0.0035 | Val: 0.0040\n","   Epoch 30 | Train: 0.0035 | Val: 0.0040\n","   Epoch 31 | Train: 0.0035 | Val: 0.0040\n","   Epoch 32 | Train: 0.0034 | Val: 0.0040\n","   Epoch 33 | Train: 0.0034 | Val: 0.0040\n","   Epoch 34 | Train: 0.0034 | Val: 0.0040\n","   Epoch 35 | Train: 0.0034 | Val: 0.0039\n","   Epoch 36 | Train: 0.0033 | Val: 0.0040\n","   Epoch 37 | Train: 0.0033 | Val: 0.0040\n","   Epoch 38 | Train: 0.0033 | Val: 0.0040\n","   Epoch 39 | Train: 0.0033 | Val: 0.0039\n","   Epoch 40 | Train: 0.0032 | Val: 0.0039\n","   Epoch 41 | Train: 0.0031 | Val: 0.0039\n","   Epoch 42 | Train: 0.0031 | Val: 0.0040\n","   Epoch 43 | Train: 0.0031 | Val: 0.0040\n","   Epoch 44 | Train: 0.0031 | Val: 0.0040\n","   Epoch 45 | Train: 0.0030 | Val: 0.0040\n","   Epoch 46 | Train: 0.0030 | Val: 0.0040\n","   Epoch 47 | Train: 0.0030 | Val: 0.0040\n","   Epoch 48 | Train: 0.0030 | Val: 0.0040\n","   Epoch 49 | Train: 0.0030 | Val: 0.0040\n","   Epoch 50 | Train: 0.0030 | Val: 0.0040\n","   üèÜ Saved best MFO model to /content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run/model_MFO.pth\n","\n","============================================================\n","üß¨ TRAINING ASPECT: CCO (1612 labels)\n","============================================================\n","   Shape mapping for CCO...\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","   Epoch 01 | Train: 0.0410 | Val: 0.0106\n","   Epoch 02 | Train: 0.0110 | Val: 0.0100\n","   Epoch 03 | Train: 0.0104 | Val: 0.0097\n","   Epoch 04 | Train: 0.0100 | Val: 0.0094\n","   Epoch 05 | Train: 0.0097 | Val: 0.0092\n","   Epoch 06 | Train: 0.0095 | Val: 0.0090\n","   Epoch 07 | Train: 0.0092 | Val: 0.0089\n","   Epoch 08 | Train: 0.0091 | Val: 0.0088\n","   Epoch 09 | Train: 0.0089 | Val: 0.0087\n","   Epoch 10 | Train: 0.0087 | Val: 0.0086\n","   Epoch 11 | Train: 0.0086 | Val: 0.0085\n","   Epoch 12 | Train: 0.0085 | Val: 0.0084\n","   Epoch 13 | Train: 0.0083 | Val: 0.0084\n","   Epoch 14 | Train: 0.0082 | Val: 0.0083\n","   Epoch 15 | Train: 0.0081 | Val: 0.0083\n","   Epoch 16 | Train: 0.0080 | Val: 0.0082\n","   Epoch 17 | Train: 0.0079 | Val: 0.0082\n","   Epoch 18 | Train: 0.0079 | Val: 0.0082\n","   Epoch 19 | Train: 0.0078 | Val: 0.0081\n","   Epoch 20 | Train: 0.0077 | Val: 0.0081\n","   Epoch 21 | Train: 0.0076 | Val: 0.0081\n","   Epoch 22 | Train: 0.0076 | Val: 0.0081\n","   Epoch 23 | Train: 0.0075 | Val: 0.0081\n","   Epoch 24 | Train: 0.0074 | Val: 0.0081\n","   Epoch 25 | Train: 0.0074 | Val: 0.0081\n","   Epoch 26 | Train: 0.0073 | Val: 0.0081\n","   Epoch 27 | Train: 0.0073 | Val: 0.0081\n","   Epoch 28 | Train: 0.0072 | Val: 0.0080\n","   Epoch 29 | Train: 0.0072 | Val: 0.0080\n","   Epoch 30 | Train: 0.0071 | Val: 0.0080\n","   Epoch 31 | Train: 0.0071 | Val: 0.0080\n","   Epoch 32 | Train: 0.0070 | Val: 0.0080\n","   Epoch 33 | Train: 0.0070 | Val: 0.0080\n","   Epoch 34 | Train: 0.0070 | Val: 0.0080\n","   Epoch 35 | Train: 0.0069 | Val: 0.0080\n","   Epoch 36 | Train: 0.0069 | Val: 0.0081\n","   Epoch 37 | Train: 0.0067 | Val: 0.0080\n","   Epoch 38 | Train: 0.0066 | Val: 0.0080\n","   Epoch 39 | Train: 0.0066 | Val: 0.0081\n","   Epoch 40 | Train: 0.0066 | Val: 0.0081\n","   Epoch 41 | Train: 0.0065 | Val: 0.0080\n","   Epoch 42 | Train: 0.0065 | Val: 0.0081\n","   Epoch 43 | Train: 0.0064 | Val: 0.0081\n","   Epoch 44 | Train: 0.0064 | Val: 0.0081\n","   Epoch 45 | Train: 0.0064 | Val: 0.0081\n","   Epoch 46 | Train: 0.0063 | Val: 0.0081\n","   Epoch 47 | Train: 0.0063 | Val: 0.0081\n","   Epoch 48 | Train: 0.0063 | Val: 0.0081\n","   Epoch 49 | Train: 0.0063 | Val: 0.0081\n","   Epoch 50 | Train: 0.0063 | Val: 0.0081\n","   üèÜ Saved best CCO model to /content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run/model_CCO.pth\n","\n","============================================================\n","üß¨ TRAINING ASPECT: BPO (4923 labels)\n","============================================================\n","   Shape mapping for BPO...\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","   Epoch 01 | Train: 0.0354 | Val: 0.0054\n","   Epoch 02 | Train: 0.0057 | Val: 0.0053\n","   Epoch 03 | Train: 0.0056 | Val: 0.0052\n","   Epoch 04 | Train: 0.0054 | Val: 0.0051\n","   Epoch 05 | Train: 0.0053 | Val: 0.0050\n","   Epoch 06 | Train: 0.0052 | Val: 0.0049\n","   Epoch 07 | Train: 0.0051 | Val: 0.0048\n","   Epoch 08 | Train: 0.0050 | Val: 0.0047\n","   Epoch 09 | Train: 0.0049 | Val: 0.0047\n","   Epoch 10 | Train: 0.0048 | Val: 0.0046\n","   Epoch 11 | Train: 0.0047 | Val: 0.0046\n","   Epoch 12 | Train: 0.0046 | Val: 0.0045\n","   Epoch 13 | Train: 0.0045 | Val: 0.0045\n","   Epoch 14 | Train: 0.0045 | Val: 0.0045\n","   Epoch 15 | Train: 0.0044 | Val: 0.0045\n","   Epoch 16 | Train: 0.0044 | Val: 0.0044\n","   Epoch 17 | Train: 0.0043 | Val: 0.0044\n","   Epoch 18 | Train: 0.0043 | Val: 0.0044\n","   Epoch 19 | Train: 0.0042 | Val: 0.0044\n","   Epoch 20 | Train: 0.0041 | Val: 0.0044\n","   Epoch 21 | Train: 0.0041 | Val: 0.0044\n","   Epoch 22 | Train: 0.0041 | Val: 0.0044\n","   Epoch 23 | Train: 0.0040 | Val: 0.0043\n","   Epoch 24 | Train: 0.0040 | Val: 0.0043\n","   Epoch 25 | Train: 0.0039 | Val: 0.0043\n","   Epoch 26 | Train: 0.0039 | Val: 0.0043\n","   Epoch 27 | Train: 0.0039 | Val: 0.0043\n","   Epoch 28 | Train: 0.0038 | Val: 0.0043\n","   Epoch 29 | Train: 0.0038 | Val: 0.0043\n","   Epoch 30 | Train: 0.0037 | Val: 0.0043\n","   Epoch 31 | Train: 0.0037 | Val: 0.0043\n","   Epoch 32 | Train: 0.0037 | Val: 0.0043\n","   Epoch 33 | Train: 0.0036 | Val: 0.0043\n","   Epoch 34 | Train: 0.0036 | Val: 0.0043\n","   Epoch 35 | Train: 0.0035 | Val: 0.0043\n","   Epoch 36 | Train: 0.0035 | Val: 0.0043\n","   Epoch 37 | Train: 0.0035 | Val: 0.0043\n","   Epoch 38 | Train: 0.0035 | Val: 0.0043\n","   Epoch 39 | Train: 0.0034 | Val: 0.0043\n","   Epoch 40 | Train: 0.0034 | Val: 0.0043\n","   Epoch 41 | Train: 0.0034 | Val: 0.0043\n","   Epoch 42 | Train: 0.0033 | Val: 0.0043\n","   Epoch 43 | Train: 0.0033 | Val: 0.0043\n","   Epoch 44 | Train: 0.0033 | Val: 0.0043\n","   Epoch 45 | Train: 0.0033 | Val: 0.0043\n","   Epoch 46 | Train: 0.0033 | Val: 0.0043\n","   Epoch 47 | Train: 0.0033 | Val: 0.0043\n","   Epoch 48 | Train: 0.0033 | Val: 0.0043\n","   Epoch 49 | Train: 0.0032 | Val: 0.0043\n","   Epoch 50 | Train: 0.0032 | Val: 0.0043\n","   üèÜ Saved best BPO model to /content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run/model_BPO.pth\n","\n","üîÆ PREDICTING WITH MULTIPLE MODELS...\n","   Loading models...\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","   Total combined labels: 10000\n"]},{"output_type":"display_data","data":{"text/plain":["Inference:   0%|          | 0/1753 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d73e0ff20f1445f8bc16261cb1f8063"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ DONE! File: /content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run/submission.tsv\n","‚úÖ Total Predictions: 4,308,204\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import gc\n","import os\n","import sys\n","from tqdm.auto import tqdm\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","CONFIG = {\n","    'EMBEDDINGS': {\n","        'ankh': {\n","            'train': '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_train_FINAL_embeddings.npy',\n","            'test':  '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_test_FINAL_embeddings.npy',\n","        },\n","        'esm': {\n","            'train': '/content/drive/MyDrive/CAFA6_Embeddings/ESM2_Aligned_Ankh/esm2_train_FINAL_embeddings.npy',\n","            'test':  '/content/drive/MyDrive/CAFA6_Embeddings/ESM2_Aligned_Ankh/esm2_test_FINAL_embeddings.npy',\n","        },\n","        'prot_t5': {\n","            'train': '/content/drive/MyDrive/CAFA6_Embeddings/Prot_T5_Aligned/protT5_xl_train_FINAL_embeddings.npy',\n","            'test':  '/content/drive/MyDrive/CAFA6_Embeddings/Prot_T5_Aligned/protT5_xl_test_FINAL_embeddings.npy',\n","        },\n","        # 'protBERT': {\n","        #     'train': '/content/drive/MyDrive/CAFA6_Embeddings/protBERT_Aligned/protBERT_train_FINAL_embeddings.npy',\n","        #     'test':  '/content/drive/MyDrive/CAFA6_Embeddings/protBERT_Aligned/protBERT_test_FINAL_embeddings.npy',\n","        # },\n","    },\n","\n","    'TRAIN_ID_PATH': '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_train_FINAL_ids.npy',\n","    'TEST_ID_PATH':  '/content/drive/MyDrive/CAFA6_Embeddings/Final_Merged/ankh_test_FINAL_ids.npy',\n","\n","    'SAVE_DIR': '/content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run',\n","    'TRAIN_TERMS': '/content/cafa6_data/Train/train_terms.tsv',\n","    'IA_FILE': '/content/cafa6_data/IA.tsv',\n","    'TEST_FASTA': '/content/cafa6_data/Test/testsuperset.fasta',\n","    'OBO_FILE': '/content/cafa6_data/Train/go-basic.obo',\n","\n","    # Model Params\n","    'TOP_K_LABELS': 10000, # Gi·ªØ m·ª©c n√†y ƒë·ªÉ model h·ªçc t·ªët\n","    # 'MIN_FREQ': 3,\n","\n","    # Input s·∫Ω l√† 768 (Ankh) + 1280 (ESM) = 2048\n","    'ENCODER_LAYERS': [2048, 1024, 512],\n","\n","    'DROPOUT_RATE': 0.4, # TƒÉng dropout v√¨ input l·ªõn\n","    'EPOCHS': 50,\n","    'BATCH_SIZE': 128,\n","    'LEARNING_RATE': 2e-4, # Gi·∫£m LR m·ªôt ch√∫t cho ·ªïn ƒë·ªãnh\n","    'LABEL_SMOOTHING': 0.1,\n","\n","    # Inference Params (Chu·∫©n 0.27 ƒëi·ªÉm)\n","    'MIN_CONFIDENCE': 0.01,\n","    'MAX_PREDS_PER_PROTEIN': 20,\n","\n","    'SEED': 42,\n","    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu'\n","}\n","\n","os.makedirs(CONFIG['SAVE_DIR'], exist_ok=True)\n","print(f\"üöÄ CAFA 6 - DUAL MODEL (ANKH + ESM) | Device: {CONFIG['DEVICE']}\")\n","\n","torch.manual_seed(CONFIG['SEED'])\n","np.random.seed(CONFIG['SEED'])\n","\n","def load_go_aspect_mapping(obo_path):\n","    print(f\"üìñ Parsing OBO file: {obo_path}...\")\n","    namespace_map = {\n","        'biological_process': 'BPO',\n","        'molecular_function': 'MFO',\n","        'cellular_component': 'CCO'\n","    }\n","    mapping = {}\n","    current_id = \"\"\n","    with open(obo_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('id: GO:'):\n","                current_id = line.split('id: ')[1]\n","            elif line.startswith('namespace:'):\n","                if current_id:\n","                    full_ns = line.split('namespace: ')[1]\n","                    mapping[current_id] = namespace_map.get(full_ns, 'UNKNOWN')\n","    print(f\"‚úÖ Loaded aspect mapping for {len(mapping):,} terms.\")\n","    return mapping\n","\n","# ============================================================================\n","# 1. MEMORY-SAFE DATASET (CH√åA KH√ìA ƒê·ªÇ KH√îNG TR√ÄN RAM)\n","# ============================================================================\n","class MultiSourceDataset(Dataset):\n","    def __init__(self, embedding_paths_dict, y_tensor=None, indices=None):\n","        self.mmaps = {}\n","        self.keys = list(embedding_paths_dict.keys())\n","\n","        # Load mmap\n","        for name, path in embedding_paths_dict.items():\n","            self.mmaps[name] = np.load(path, mmap_mode='r')\n","\n","        # Base length\n","        first_key = self.keys[0]\n","        self.total_len = len(self.mmaps[first_key])\n","\n","        self.indices = indices if indices is not None else np.arange(self.total_len)\n","        self.y = y_tensor\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        real_idx = self.indices[idx]\n","\n","        inputs = []\n","        # Load t·ª´ng vector -> List of Tensors\n","        for key in self.keys:\n","            vec = torch.from_numpy(self.mmaps[key][real_idx].copy()).float()\n","            inputs.append(vec)\n","\n","        if self.y is not None:\n","            return inputs, self.y[real_idx]\n","        return (inputs,)\n","\n","# ============================================================================\n","# 2. LOAD DATA & PROCESS LABELS\n","# ============================================================================\n","print(\"\\n[1/5] Checking Files...\")\n","train_ids = np.load(CONFIG['TRAIN_ID_PATH'])\n","print(f\"   ‚úì Train IDs: {len(train_ids)}\")\n","\n","print(\"\\n[2/5] Processing Labels (IA Strategy)...\")\n","# 1. Load Terms & IA\n","df_terms = pd.read_csv(CONFIG['TRAIN_TERMS'], sep='\\t', header=0, names=['EntryID', 'term', 'aspect'])\n","df_ia = pd.read_csv(CONFIG['IA_FILE'], sep='\\t', names=['term', 'ia'])\n","ia_dict = dict(zip(df_ia['term'], df_ia['ia']))\n","\n","# 2. T√≠nh Score\n","term_counts = df_terms['term'].value_counts().reset_index()\n","term_counts.columns = ['term', 'freq']\n","\n","# print(f\"   Original terms count: {len(term_counts)}\")\n","# term_counts = term_counts[term_counts['freq'] >= CONFIG['MIN_FREQ']]\n","# print(f\"   -> After filtering (Freq >= {CONFIG['MIN_FREQ']}): {len(term_counts)} terms\")\n","\n","term_counts['ia'] = term_counts['term'].map(ia_dict).fillna(0.0)\n","term_counts['score'] = term_counts['freq'] * term_counts['ia']\n","\n","# 3. Ch·ªçn Top K\n","top_terms_df = term_counts.sort_values(by='score', ascending=False).head(CONFIG['TOP_K_LABELS'])\n","top_terms = top_terms_df['term'].tolist()\n","print(f\"   ‚úì Selected {len(top_terms)} terms.\")\n","\n","obo_path = CONFIG['OBO_FILE']\n","go_aspect_map = load_go_aspect_mapping(obo_path)\n","\n","aspect_groups = {\n","    'MFO': [],\n","    'BPO': [],\n","    'CCO': []\n","}\n","\n","for term in top_terms:\n","    aspect = go_aspect_map.get(term, 'UNKNOWN')\n","    if aspect in aspect_groups:\n","        aspect_groups[aspect].append(term)\n","\n","print(f\"üìä Aspect Distribution:\")\n","print(f\"   - MFO (Molecular Function): {len(aspect_groups['MFO']):,} labels\")\n","print(f\"   - CCO (Cellular Component): {len(aspect_groups['CCO']):,} labels\")\n","print(f\"   - BPO (Biological Process): {len(aspect_groups['BPO']):,} labels\")\n","\n","# 4. Map ID -> Terms\n","df_filtered = df_terms[df_terms['term'].isin(top_terms)]\n","id_to_terms = df_filtered.groupby('EntryID')['term'].apply(list).to_dict()\n","\n","# D·ªçn d·∫πp\n","del df_terms, df_filtered, df_ia, term_counts, top_terms_df\n","gc.collect()\n","\n","# ============================================================================\n","# 5. MODEL & TRAINING\n","# ============================================================================\n","print(\"\\n[5/5] Building Model...\")\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, in_dim, layers_config, dropout):\n","        super().__init__()\n","        layers = []\n","\n","        # 1. LAYER NORM ƒê·∫¶U V√ÄO (B·∫ÆT BU·ªòC)\n","        # ƒê·ªÉ c√¢n b·∫±ng '√¢m l∆∞·ª£ng' gi·ªØa ESM (h√©t to) v√† Ankh (n√≥i nh·ªè)\n","        layers.append(nn.LayerNorm(in_dim))\n","\n","        prev_dim = in_dim\n","        for i, dim in enumerate(layers_config):\n","            layers.extend([\n","                nn.Linear(prev_dim, dim),\n","                nn.BatchNorm1d(dim), # ·ªîn ƒë·ªãnh training\n","                nn.GELU(),           # Hi·ªán ƒë·∫°i h∆°n ReLU\n","                nn.Dropout(dropout)\n","            ])\n","            prev_dim = dim\n","\n","        self.net = nn.Sequential(*layers)\n","        self.out_dim = prev_dim\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Module l·ªçc nhi·ªÖu (Attention)\n","class SEBlock(nn.Module):\n","    \"\"\"Squeeze-and-Excitation Block ƒë·ªÉ l·ªçc nhi·ªÖu sau khi g·ªôp\"\"\"\n","    def __init__(self, in_dim, reduction=4):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(in_dim, in_dim // reduction, bias=False),\n","            nn.ReLU(),\n","            nn.Linear(in_dim // reduction, in_dim, bias=False),\n","            nn.Sigmoid() # T·∫°o ra mask t·ª´ 0 ƒë·∫øn 1\n","        )\n","\n","    def forward(self, x):\n","        # x shape: [Batch, Dim]\n","        # Attention weight: [Batch, Dim]\n","        w = self.fc(x)\n","        # Nh√¢n tr·ªçng s·ªë v√†o x: C√°i n√†o quan tr·ªçng th√¨ gi·ªØ, r√°c th√¨ nh√¢n v·ªõi 0\n","        return x * w\n","\n","class MultiModalNet(nn.Module):\n","    def __init__(self, input_dims_list, encoder_layers, dropout, num_classes):\n","        super().__init__()\n","\n","        self.encoders = nn.ModuleList()\n","        self.fusion_input_dim = 0\n","\n","        print(\"\\nüèóÔ∏è Building Advanced Architecture:\")\n","\n","        # 1. X√¢y d·ª±ng c√°c nh√°nh Encoder\n","        for i, in_dim in enumerate(input_dims_list):\n","            print(f\"   ‚û§ Branch {i+1}: Input {in_dim} -> Output {encoder_layers[-1]}\")\n","            enc = EncoderBlock(in_dim, encoder_layers, dropout)\n","            self.encoders.append(enc)\n","            self.fusion_input_dim += enc.out_dim\n","\n","        print(f\"   ‚û§ Fusion Dim: {self.fusion_input_dim}\")\n","\n","        # 2. SE-Block (B·ªô l·ªçc th√¥ng minh)\n","        self.attention_filter = SEBlock(self.fusion_input_dim)\n","\n","        # 3. Layer t·ªïng h·ª£p cu·ªëi c√πng\n","        self.head = nn.Sequential(\n","            nn.BatchNorm1d(self.fusion_input_dim),\n","            nn.Linear(self.fusion_input_dim, 512),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","\n","            # Th√™m m·ªôt l·ªõp n·ªØa ƒë·ªÉ tƒÉng kh·∫£ nƒÉng h·ªçc\n","            nn.Linear(512, 512),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","\n","            nn.Linear(512, num_classes)\n","        )\n","\n","    def forward(self, inputs_list):\n","        features = []\n","        # ƒêi qua t·ª´ng nh√°nh\n","        for i, encoder in enumerate(self.encoders):\n","            feat = encoder(inputs_list[i])\n","            features.append(feat)\n","\n","        # G·ªôp l·∫°i (Concatenate)\n","        combined = torch.cat(features, dim=1)\n","\n","        # L·ªåC NHI·ªÑU (ƒêi·ªÉm kh√°c bi·ªát l·ªõn nh·∫•t)\n","        # M·∫°ng s·∫Ω t·ª± h·ªçc c√°ch \"t·∫Øt ti·∫øng\" c√°c ƒë·∫∑c tr∆∞ng r√°c t·ª´ ProtT5 n·∫øu n√≥ th·∫•y kh√¥ng c·∫ßn thi·∫øt\n","        refined = self.attention_filter(combined)\n","\n","        return self.head(refined)\n","\n","train_paths = {k: v['train'] for k, v in CONFIG['EMBEDDINGS'].items()}\n","aspect_names = ['MFO', 'CCO', 'BPO']\n","temp_dataset = MultiSourceDataset(train_paths, indices=[0])\n","sample_inputs = temp_dataset[0][0] # Get inputs of first sample\n","INPUT_DIMS_LIST = [x.shape[0] for x in sample_inputs]\n","print(f\"   ‚úì Detected Input Dims: {INPUT_DIMS_LIST}\")\n","del temp_dataset\n","\n","# L∆∞u th√¥ng tin ƒë·ªÉ d√πng cho Inference\n","trained_models_meta = []\n","\n","for aspect in aspect_names:\n","    terms = aspect_groups[aspect]\n","    if len(terms) == 0: continue\n","\n","    print(f\"\\n\" + \"=\"*60)\n","    print(f\"üß¨ TRAINING ASPECT: {aspect} ({len(terms)} labels)\")\n","    print(\"=\"*60)\n","\n","    # --- A. PREPARE Y TENSOR CHO ASPECT N√ÄY ---\n","    print(f\"   Shape mapping for {aspect}...\")\n","    mlb = MultiLabelBinarizer(classes=terms, sparse_output=True)\n","    mlb.fit([terms])\n","\n","    # Ch·ªâ transform c√°c nh√£n thu·ªôc Aspect n√†y\n","    # L∆∞u √Ω: id_to_terms ch·ª©a t·∫•t c·∫£, mlb s·∫Ω t·ª± ignore c√°c nh√£n kh√¥ng thu·ªôc 'terms'\n","    import warnings\n","    with warnings.catch_warnings():\n","        warnings.simplefilter(\"ignore\")\n","        y_labels_list = [id_to_terms.get(pid, []) for pid in train_ids]\n","        y_sparse = mlb.transform(y_labels_list)\n","\n","    # Convert to Tensor\n","    y_binary = y_sparse.astype(np.float32).toarray()\n","    y_tensor = torch.from_numpy(y_binary)\n","\n","    # Label Smoothing\n","    if CONFIG['LABEL_SMOOTHING'] > 0:\n","        y_tensor.mul_(1 - CONFIG['LABEL_SMOOTHING']).add_(CONFIG['LABEL_SMOOTHING'] / len(terms))\n","\n","    # Pos Weight (Optional - Re-calculate per aspect)\n","    weights_list = [ia_dict.get(t, 0.0) for t in terms]\n","    pos_weight = torch.tensor(weights_list, dtype=torch.float32).to(CONFIG['DEVICE'])\n","\n","    # --- B. DATALOADERS ---\n","    indices = np.arange(len(y_tensor))\n","    train_idx, val_idx = train_test_split(indices, test_size=0.15, random_state=CONFIG['SEED'])\n","\n","    train_dataset = MultiSourceDataset(train_paths, y_tensor, indices=train_idx)\n","    val_dataset   = MultiSourceDataset(train_paths, y_tensor, indices=val_idx)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\n","    val_loader   = DataLoader(val_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n","\n","    # Auto detect input dims (Ch·ªâ l√†m l·∫ßn ƒë·∫ßu ho·∫∑c l√†m l·∫°i c≈©ng ƒë∆∞·ª£c)\n","    # sample_inputs, _ = train_dataset[0]\n","    # INPUT_DIMS_LIST = [x.shape[0] for x in sample_inputs]\n","\n","    # --- C. BUILD MODEL ---\n","    # Reset model m·ªõi cho m·ªói Aspect\n","    # model = MultiModalNet(\n","    #     input_dims_list = INPUT_DIMS_LIST,\n","    #     encoder_layers  = CONFIG['ENCODER_LAYERS'],\n","    #     dropout         = CONFIG['DROPOUT_RATE'],\n","    #     num_classes     = len(terms) # Output dim thay ƒë·ªïi theo Aspect\n","    # ).to(CONFIG['DEVICE'])\n","\n","    # criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    # optimizer = optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'], weight_decay=0.01)\n","    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","\n","    # # --- D. TRAIN ---\n","    # best_val_loss = float('inf')\n","    model_save_path = f\"{CONFIG['SAVE_DIR']}/model_{aspect}.pth\"\n","\n","    # for epoch in range(CONFIG['EPOCHS']):\n","    #     model.train()\n","    #     train_loss = 0\n","    #     for X_b, y_b in train_loader:\n","    #         X_b, y_b = [x.to(CONFIG['DEVICE']) for x in X_b], y_b.to(CONFIG['DEVICE'])\n","    #         optimizer.zero_grad()\n","    #         logits = model(X_b)\n","    #         loss = criterion(logits, y_b)\n","    #         loss.backward()\n","    #         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    #         optimizer.step()\n","    #         train_loss += loss.item()\n","\n","    #     avg_train = train_loss / len(train_loader)\n","\n","    #     model.eval()\n","    #     val_loss = 0\n","    #     with torch.no_grad():\n","    #         for X_b, y_b in val_loader:\n","    #             X_b, y_b = [x.to(CONFIG['DEVICE']) for x in X_b], y_b.to(CONFIG['DEVICE'])\n","    #             logits = model(X_b)\n","    #             loss = criterion(logits, y_b)\n","    #             val_loss += loss.item()\n","\n","    #     avg_val = val_loss / len(val_loader)\n","    #     scheduler.step(avg_val)\n","\n","    #     print(f\"   Epoch {epoch+1:02d} | Train: {avg_train:.4f} | Val: {avg_val:.4f}\")\n","\n","    #     if avg_val < best_val_loss:\n","    #         best_val_loss = avg_val\n","    #         torch.save(model.state_dict(), model_save_path)\n","\n","    # print(f\"   üèÜ Saved best {aspect} model to {model_save_path}\")\n","\n","    # L∆∞u meta data\n","    trained_models_meta.append({\n","        'aspect': aspect,\n","        'path': model_save_path,\n","        'terms': terms,\n","        'num_classes': len(terms)\n","    })\n","\n","    # D·ªçn d·∫πp RAM\n","    # del model, train_loader, val_loader, train_dataset, val_dataset, y_tensor, y_sparse\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# ============================================================================\n","# 4. INFERENCE (MULTI-MODEL MERGE)\n","# ============================================================================\n","print(\"\\nüîÆ PREDICTING WITH MULTIPLE MODELS...\")\n","\n","QUOTAS = {\n","    'MFO': 5,  # Molecular Function th∆∞·ªùng √≠t nh√£n h∆°n nh∆∞ng ƒë·ªô ch√≠nh x√°c cao\n","    'BPO': 12,  # Biological Process r·∫•t nhi·ªÅu nh√£n, c·∫ßn l·∫•y nhi·ªÅu h∆°n\n","    'CCO': 3   # Cellular Component √≠t nh√£n\n","}\n","\n","test_paths = {k: v['test'] for k, v in CONFIG['EMBEDDINGS'].items()}\n","test_ids = np.load(CONFIG['TEST_ID_PATH'])\n","\n","# Dataset Test (D√πng chung cho c·∫£ 3 model)\n","test_dataset = MultiSourceDataset(test_paths)\n","test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE']*2, shuffle=False, num_workers=2)\n","\n","# Load 3 Model v√†o list (Deep Learning model nh·∫π, load c·∫£ 3 v√†o VRAM ok)\n","# N·∫øu VRAM qu√° y·∫øu (T4 th∆∞·ªùng ch·ªãu ƒë∆∞·ª£c), th√¨ c√≥ th·ªÉ load-predict-unload t·ª´ng c√°i\n","models_dict = {}\n","\n","print(\"   Loading models...\")\n","for meta in trained_models_meta:\n","    # Build l·∫°i ki·∫øn tr√∫c\n","    aspect = meta['aspect']\n","    m = MultiModalNet(\n","        input_dims_list = INPUT_DIMS_LIST,\n","        encoder_layers  = CONFIG['ENCODER_LAYERS'],\n","        dropout         = CONFIG['DROPOUT_RATE'],\n","        num_classes     = meta['num_classes']\n","    ).to(CONFIG['DEVICE'])\n","\n","    m.load_state_dict(torch.load(meta['path']))\n","    m.eval()\n","    models_dict[aspect] = {\n","        'model': m,\n","        'terms': meta['terms'] # Danh s√°ch nh√£n ri√™ng c·ªßa aspect ƒë√≥\n","    }\n","\n","print(f\"   Loaded models for: {list(models_dict.keys())}\")\n","\n","submission_path = f\"{CONFIG['SAVE_DIR']}/submission.tsv\"\n","n_predictions = 0\n","current_idx = 0\n","\n","with open(submission_path, 'w') as f:\n","    with torch.no_grad():\n","        for batch_idx, (X_b,) in enumerate(tqdm(test_loader, desc=\"Inferring\")):\n","            X_b = [x.to(CONFIG['DEVICE']) for x in X_b]\n","\n","            # L·∫•y ID c·ªßa batch hi·ªán t·∫°i\n","            start_idx = batch_idx * (CONFIG['BATCH_SIZE']*2)\n","            end_idx = start_idx + X_b[0].shape[0] # D√πng shape c·ªßa tensor ƒë·∫ßu ti√™n\n","            ids_batch = test_ids[start_idx : end_idx]\n","\n","            # Dictionary l∆∞u k·∫øt qu·∫£ t·∫°m: {pid: [(term, score), ...]}\n","            batch_results = {pid: [] for pid in ids_batch}\n","\n","            # --- CH·∫†Y T·ª™NG ASPECT RI√äNG BI·ªÜT ---\n","            for aspect, item in models_dict.items():\n","                model = item['model']\n","                terms_list = item['terms']\n","                quota = QUOTAS.get(aspect, 15) # L·∫•y quota, m·∫∑c ƒë·ªãnh 15\n","\n","                # Predict\n","                logits = model(X_b)\n","                probs = torch.sigmoid(logits).cpu().numpy()\n","\n","                # L·∫•y Top-K CHO T·ª™NG ASPECT\n","                for i, pid in enumerate(ids_batch):\n","                    p_row = probs[i]\n","\n","                    # L·∫•y Top K c·ªßa aspect n√†y\n","                    if len(p_row) > quota:\n","                        ind = np.argpartition(p_row, -quota)[-quota:]\n","                    else:\n","                        ind = np.arange(len(p_row))\n","\n","                    # Sort l·∫°i\n","                    ind = ind[np.argsort(p_row[ind])][::-1]\n","\n","                    # L∆∞u v√†o buffer\n","                    for idx in ind:\n","                        score = p_row[idx]\n","                        if score > CONFIG['MIN_CONFIDENCE']:\n","                            term_name = terms_list[idx]\n","                            # L∆∞u tuple (term, score)\n","                            batch_results[pid].append((term_name, score))\n","\n","            # --- GHI FILE ---\n","            for pid in ids_batch:\n","                # L√∫c n√†y batch_results[pid] ch·ª©a h·ªón h·ª£p MFO, BPO, CCO\n","                # Ch√∫ng ta KH√îNG sort l·∫°i theo ƒëi·ªÉm n·ªØa (ƒë·ªÉ t√¥n tr·ªçng quota)\n","                # Ho·∫∑c c√≥ th·ªÉ sort nh·∫π ƒë·ªÉ file ƒë·∫πp h∆°n, nh∆∞ng quan tr·ªçng l√† ƒë√£ l·∫•y ƒë·ªß s·ªë l∆∞·ª£ng\n","\n","                preds = batch_results[pid]\n","                # Optional: Sort t·ªïng th·ªÉ l·∫ßn cu·ªëi ƒë·ªÉ file submission ƒë·∫πp\n","                preds.sort(key=lambda x: x[1], reverse=True)\n","\n","                for term, score in preds:\n","                    f.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n","                    n_predictions += 1\n","\n","print(f\"\\n‚úÖ DONE! Optimized Submission: {submission_path}\")\n","print(f\"‚úÖ Total Predictions: {n_predictions:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6401a4edd126468fba867d0e0d65e433","378321947ee94b8c99d185c913e3a1e2","c1f48ac0771242ec9e80d3fc49b2cba7","829c9f22f314420eb31e888d24b3e784","d657fb971c204ec2aed9f931372abf5f","8cca2a1a7c5f4177b301d64f4e21e8f4","4da7acbb9b824f8fbae4fc00b573f534","8a8efe6359ee471bb8d82903b7acb83c","117d474eae5f4e54b316296b12ba3f3b","5596e3b6259a45cc9b22a43f00ee3b1c","fc6addaaf5e14a23a26b70d080dcb073"]},"id":"YtzEEC7xNIKV","executionInfo":{"status":"ok","timestamp":1765859391873,"user_tz":-420,"elapsed":77062,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"ad72ab7d-ed6e-4a66-be5a-aad5c486e316"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ CAFA 6 - DUAL MODEL (ANKH + ESM) | Device: cuda\n","\n","[1/5] Checking Files...\n","   ‚úì Train IDs: 82404\n","\n","[2/5] Processing Labels (IA Strategy)...\n","   ‚úì Selected 10000 terms.\n","üìñ Parsing OBO file: /content/cafa6_data/Train/go-basic.obo...\n","‚úÖ Loaded aspect mapping for 48,101 terms.\n","üìä Aspect Distribution:\n","   - MFO (Molecular Function): 3,465 labels\n","   - CCO (Cellular Component): 1,612 labels\n","   - BPO (Biological Process): 4,923 labels\n","\n","[5/5] Building Model...\n","   ‚úì Detected Input Dims: [768, 1280, 1024]\n","\n","============================================================\n","üß¨ TRAINING ASPECT: MFO (3465 labels)\n","============================================================\n","   Shape mapping for MFO...\n","\n","============================================================\n","üß¨ TRAINING ASPECT: CCO (1612 labels)\n","============================================================\n","   Shape mapping for CCO...\n","\n","============================================================\n","üß¨ TRAINING ASPECT: BPO (4923 labels)\n","============================================================\n","   Shape mapping for BPO...\n","\n","üîÆ PREDICTING WITH MULTIPLE MODELS...\n","   Loading models...\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","\n","üèóÔ∏è Building Advanced Architecture:\n","   ‚û§ Branch 1: Input 768 -> Output 512\n","   ‚û§ Branch 2: Input 1280 -> Output 512\n","   ‚û§ Branch 3: Input 1024 -> Output 512\n","   ‚û§ Fusion Dim: 1536\n","   Loaded models for: ['MFO', 'CCO', 'BPO']\n"]},{"output_type":"display_data","data":{"text/plain":["Inferring:   0%|          | 0/877 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6401a4edd126468fba867d0e0d65e433"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ DONE! Optimized Submission: /content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run/submission.tsv\n","‚úÖ Total Predictions: 3,872,236\n"]}]},{"cell_type":"code","source":["# Ch·∫°y l·ªánh n√†y trong m·ªôt cell m·ªõi\n","!kaggle competitions submit \\\n","    -c cafa-6-protein-function-prediction \\\n","    -f /content/drive/MyDrive/CAFA6_Results/prott5_esm2_ankh_Run/submission.tsv \\\n","    -m \"esm2 ankh prot t5 chunk_aspect mf 5 bp 12 cc 3\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEnzkP3GpCub","executionInfo":{"status":"ok","timestamp":1765860105204,"user_tz":-420,"elapsed":3945,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"89aa3de1-4235-4f89-cea1-1e6cb9cb7376"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 88.9M/88.9M [00:01<00:00, 50.4MB/s]\n","Successfully submitted to CAFA 6 Protein Function Prediction"]}]},{"cell_type":"code","source":["!kaggle competitions submissions -c cafa-6-protein-function-prediction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RphLfaympGce","executionInfo":{"status":"ok","timestamp":1765860310991,"user_tz":-420,"elapsed":496,"user":{"displayName":"Hung Nguyen","userId":"06361922292825162630"}},"outputId":"240b17e0-c1ff-407d-d7a8-cb825f9161f4"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["fileName        date                        description                                                            status                     publicScore  privateScore  \n","--------------  --------------------------  ---------------------------------------------------------------------  -------------------------  -----------  ------------  \n","submission.tsv  2025-12-16 04:41:43.003000  esm2 ankh prot t5 chunk_aspect mf 5 bp 12 cc 3                         SubmissionStatus.COMPLETE  0.268                      \n","submission.tsv  2025-12-16 04:35:42.487000  esm2 ankh prot t5 chunk_aspect mf 5 bp 12 cc 3                         SubmissionStatus.PENDING                              \n","submission.tsv  2025-12-15 04:33:37.193000  esm2 ankh prot t5 best_loss_model_path seed=17                         SubmissionStatus.COMPLETE  0.280                      \n","submission.tsv  2025-12-15 04:07:58.467000  esm2 ankh prot t5 best_loss_model_path top_k_labels=10k test_size=0.1  SubmissionStatus.COMPLETE  0.263                      \n","submission.tsv  2025-12-15 03:42:38.793000  esm2 ankh prot t5 best_loss_model_path top_k_labels=10k                SubmissionStatus.COMPLETE  0.303                      \n","submission.tsv  2025-12-15 03:16:22.497000  esm2 ankh prot t5 best_loss_model_path top_k_labels=15k                SubmissionStatus.COMPLETE  0.266                      \n","submission.tsv  2025-12-14 16:52:26.390000  esm2 ankh prot t5 best_loss_model_path top_k_labels=5k                 SubmissionStatus.COMPLETE  0.259                      \n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NKRSdywDzTxR"},"execution_count":null,"outputs":[]}]}